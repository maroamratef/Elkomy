{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXfutajwyaLZ",
        "outputId": "f25cd8b0-02ec-4309-e80b-d7fe48d27762"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SAMMD'...\n",
            "remote: Enumerating objects: 155, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 155 (delta 38), reused 27 (delta 27), pack-reused 110 (from 1)\u001b[K\n",
            "Receiving objects: 100% (155/155), 324.78 KiB | 5.32 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/SAMMD/SAMMD\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/drive\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 70.4MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182M/182M [00:30<00:00, 6.05MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Start Training, Resnet-18!\n",
            "\n",
            "Epoch: 1\n",
            "[epoch:1, iter:1] Loss: 2.384 | Acc: 15.625% \n",
            "[epoch:1, iter:2] Loss: 2.358 | Acc: 14.453% \n",
            "[epoch:1, iter:3] Loss: 2.324 | Acc: 17.188% \n",
            "[epoch:1, iter:4] Loss: 2.314 | Acc: 15.625% \n",
            "[epoch:1, iter:5] Loss: 2.295 | Acc: 15.469% \n",
            "[epoch:1, iter:6] Loss: 2.280 | Acc: 15.234% \n",
            "[epoch:1, iter:7] Loss: 2.282 | Acc: 14.397% \n",
            "[epoch:1, iter:8] Loss: 2.251 | Acc: 14.941% \n",
            "[epoch:1, iter:9] Loss: 2.219 | Acc: 15.799% \n",
            "[epoch:1, iter:10] Loss: 2.199 | Acc: 16.406% \n",
            "[epoch:1, iter:11] Loss: 2.177 | Acc: 17.401% \n",
            "[epoch:1, iter:12] Loss: 2.157 | Acc: 18.034% \n",
            "[epoch:1, iter:13] Loss: 2.136 | Acc: 19.111% \n",
            "[epoch:1, iter:14] Loss: 2.113 | Acc: 19.699% \n",
            "[epoch:1, iter:15] Loss: 2.103 | Acc: 20.260% \n",
            "[epoch:1, iter:16] Loss: 2.097 | Acc: 20.508% \n",
            "[epoch:1, iter:17] Loss: 2.074 | Acc: 21.645% \n",
            "[epoch:1, iter:18] Loss: 2.062 | Acc: 22.005% \n",
            "[epoch:1, iter:19] Loss: 2.053 | Acc: 22.492% \n",
            "[epoch:1, iter:20] Loss: 2.039 | Acc: 23.047% \n",
            "[epoch:1, iter:21] Loss: 2.030 | Acc: 23.624% \n",
            "[epoch:1, iter:22] Loss: 2.015 | Acc: 24.183% \n",
            "[epoch:1, iter:23] Loss: 1.998 | Acc: 24.966% \n",
            "[epoch:1, iter:24] Loss: 1.992 | Acc: 25.228% \n",
            "[epoch:1, iter:25] Loss: 1.986 | Acc: 25.531% \n",
            "[epoch:1, iter:26] Loss: 1.977 | Acc: 25.962% \n",
            "[epoch:1, iter:27] Loss: 1.977 | Acc: 26.100% \n",
            "[epoch:1, iter:28] Loss: 1.967 | Acc: 26.590% \n",
            "[epoch:1, iter:29] Loss: 1.963 | Acc: 26.589% \n",
            "[epoch:1, iter:30] Loss: 1.949 | Acc: 27.031% \n",
            "[epoch:1, iter:31] Loss: 1.944 | Acc: 27.319% \n",
            "[epoch:1, iter:32] Loss: 1.939 | Acc: 27.661% \n",
            "[epoch:1, iter:33] Loss: 1.929 | Acc: 28.101% \n",
            "[epoch:1, iter:34] Loss: 1.923 | Acc: 28.447% \n",
            "[epoch:1, iter:35] Loss: 1.911 | Acc: 28.862% \n",
            "[epoch:1, iter:36] Loss: 1.903 | Acc: 29.253% \n",
            "[epoch:1, iter:37] Loss: 1.898 | Acc: 29.497% \n",
            "[epoch:1, iter:38] Loss: 1.891 | Acc: 29.811% \n",
            "[epoch:1, iter:39] Loss: 1.889 | Acc: 29.888% \n"
          ]
        }
      ],
      "source": [
        "# Clone the SAMMD repository from GitHub\n",
        "!git clone https://github.com/Sjtubrian/SAMMD.git\n",
        "\n",
        "# Change the working directory to the cloned repository\n",
        "%cd SAMMD\n",
        "\n",
        "# Install the required dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Mount Google Drive for storing data or results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Step 1: Download CIFAR-10 and SVHN datasets\n",
        "# CIFAR-10 Dataset\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# SVHN Dataset\n",
        "svhn = datasets.SVHN(root='./data', split='train', download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Step 2: Running the training script (train_model.py)\n",
        "# Now executing the script from the correct path: adv/train_model.py\n",
        "!python3 adv/train_model.py\n",
        "\n",
        "# Step 3: Generating Adversarial Data (generator.py)\n",
        "# Running the adversarial data generation script from the correct path: adv/generator.py\n",
        "!python3 adv/generator.py\n",
        "\n",
        "# Ensure results or adversarial data are saved to your Google Drive or local environment\n",
        "# For example, if you need to store results in Google Drive\n",
        "output_dir = '/content/drive/My Drive/adversarial_data'\n"
      ]
    }
  ]
}