{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Input protein sequence(s), then hit `Runtime` -> `Run all`\n",
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "query_sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK' #@param {type:\"string\"}\n",
        "#@markdown  - Use `:` to specify inter-protein chainbreaks for **modeling complexes** (supports homo- and hetro-oligomers). For example **PI...SK:PI...SK** for a homodimer\n",
        "jobname = 'test' #@param {type:\"string\"}\n",
        "# number of models to use\n",
        "num_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\n",
        "#@markdown - specify how many of the top ranked structures to relax using amber\n",
        "template_mode = \"none\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
        "#@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))\n",
        "\n",
        "use_amber = num_relax > 0\n",
        "\n",
        "# remove whitespaces\n",
        "query_sequence = \"\".join(query_sequence.split())\n",
        "\n",
        "basejobname = \"\".join(jobname.split())\n",
        "basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "jobname = add_hash(basejobname, query_sequence)\n",
        "\n",
        "# check if directory with jobname exists\n",
        "def check(folder):\n",
        "  if os.path.exists(folder):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "if not check(jobname):\n",
        "  n = 0\n",
        "  while not check(f\"{jobname}_{n}\"): n += 1\n",
        "  jobname = f\"{jobname}_{n}\"\n",
        "\n",
        "# make directory to save results\n",
        "os.makedirs(jobname, exist_ok=True)\n",
        "\n",
        "# save queries\n",
        "queries_path = os.path.join(jobname, f\"{jobname}.csv\")\n",
        "with open(queries_path, \"w\") as text_file:\n",
        "  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "\n",
        "if template_mode == \"pdb100\":\n",
        "  use_templates = True\n",
        "  custom_template_path = None\n",
        "elif template_mode == \"custom\":\n",
        "  custom_template_path = os.path.join(jobname,f\"template\")\n",
        "  os.makedirs(custom_template_path, exist_ok=True)\n",
        "  uploaded = files.upload()\n",
        "  use_templates = True\n",
        "  for fn in uploaded.keys():\n",
        "    os.rename(fn,os.path.join(custom_template_path,fn))\n",
        "else:\n",
        "  custom_template_path = None\n",
        "  use_templates = False\n",
        "\n",
        "print(\"jobname\",jobname)\n",
        "print(\"sequence\",query_sequence)\n",
        "print(\"length\",len(query_sequence.replace(\":\",\"\")))"
      ],
      "metadata": {
        "id": "X8tF0lDyBJuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "%%time\n",
        "import os\n",
        "USE_AMBER = use_amber\n",
        "USE_TEMPLATES = use_templates\n",
        "PYTHON_VERSION = python_version\n",
        "\n",
        "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
        "  print(\"installing colabfold...\")\n",
        "  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
        "  if os.environ.get('TPU_NAME', False) != False:\n",
        "    os.system(\"pip uninstall -y jax jaxlib\")\n",
        "    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
        "  os.system(\"touch COLABFOLD_READY\")\n",
        "\n",
        "if USE_AMBER or USE_TEMPLATES:\n",
        "  if not os.path.isfile(\"CONDA_READY\"):\n",
        "    print(\"installing conda...\")\n",
        "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
        "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
        "    os.system(\"mamba config --set auto_update_conda false\")\n",
        "    os.system(\"touch CONDA_READY\")\n",
        "\n",
        "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "  print(\"installing hhsuite and amber...\")\n",
        "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "  os.system(\"touch HH_READY\")\n",
        "  os.system(\"touch AMBER_READY\")\n",
        "else:\n",
        "  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
        "    print(\"installing hhsuite...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
        "    os.system(\"touch HH_READY\")\n",
        "  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "    print(\"installing amber...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "    os.system(\"touch AMBER_READY\")"
      ],
      "metadata": {
        "id": "5gCgqqh0BRbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### MSA options (custom MSA upload, single sequence, pairing mode)\n",
        "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
        "pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
        "#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
        "\n",
        "# decide which a3m to use\n",
        "if \"mmseqs2\" in msa_mode:\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n",
        "\n",
        "elif msa_mode == \"custom\":\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n",
        "  if not os.path.isfile(a3m_file):\n",
        "    custom_msa_dict = files.upload()\n",
        "    custom_msa = list(custom_msa_dict.keys())[0]\n",
        "    header = 0\n",
        "    import fileinput\n",
        "    for line in fileinput.FileInput(custom_msa,inplace=1):\n",
        "      if line.startswith(\">\"):\n",
        "         header = header + 1\n",
        "      if not line.rstrip():\n",
        "        continue\n",
        "      if line.startswith(\">\") == False and header == 1:\n",
        "         query_sequence = line.rstrip()\n",
        "      print(line, end='')\n",
        "\n",
        "    os.rename(custom_msa, a3m_file)\n",
        "    queries_path=a3m_file\n",
        "    print(f\"moving {custom_msa} to {a3m_file}\")\n",
        "\n",
        "else:\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)"
      ],
      "metadata": {
        "id": "7KM8GNxKBS3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Advanced settings\n",
        "model_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\"]\n",
        "#@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
        "#@markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
        "num_recycles = \"3\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
        "#@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
        "recycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
        "#@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
        "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
        "#@markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)\n",
        "pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
        "#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
        "\n",
        "\n",
        "#@markdown #### Sample settings\n",
        "#@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
        "#@markdown -  decrease `max_msa` to increase uncertainity\n",
        "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
        "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
        "use_dropout = False #@param {type:\"boolean\"}\n",
        "\n",
        "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
        "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
        "if max_msa == \"auto\": max_msa = None\n",
        "\n",
        "#@markdown #### Save settings\n",
        "save_all = False #@param {type:\"boolean\"}\n",
        "save_recycles = False #@param {type:\"boolean\"}\n",
        "save_to_google_drive = False #@param {type:\"boolean\"}\n",
        "#@markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive\n",
        "dpi = 200 #@param {type:\"integer\"}\n",
        "#@markdown - set dpi for image resolution\n",
        "\n",
        "if save_to_google_drive:\n",
        "  from pydrive2.drive import GoogleDrive\n",
        "  from pydrive2.auth import GoogleAuth\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  print(\"You are logged into Google Drive and are good to go!\")\n",
        "\n",
        "#@markdown Don't forget to hit `Runtime` -> `Run all` after updating the form."
      ],
      "metadata": {
        "id": "RZrJ8ddqBlTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Prediction\n",
        "display_images = True #@param {type:\"boolean\"}\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from Bio import BiopythonDeprecationWarning\n",
        "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
        "from pathlib import Path\n",
        "from colabfold.download import download_alphafold_params, default_data_dir\n",
        "from colabfold.utils import setup_logging\n",
        "from colabfold.batch import get_queries, run, set_model_type\n",
        "from colabfold.plot import plot_msa_v2\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "try:\n",
        "  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
        "except:\n",
        "  K80_chk = \"0\"\n",
        "  pass\n",
        "if \"1\" in K80_chk:\n",
        "  print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
        "  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
        "    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
        "  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
        "    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
        "\n",
        "from colabfold.colabfold import plot_protein\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For some reason we need that to get pdbfixer to import\n",
        "if use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
        "    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
        "\n",
        "def input_features_callback(input_features):\n",
        "  if display_images:\n",
        "    plot_msa_v2(input_features)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def prediction_callback(protein_obj, length,\n",
        "                        prediction_result, input_features, mode):\n",
        "  model_name, relaxed = mode\n",
        "  if not relaxed:\n",
        "    if display_images:\n",
        "      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "result_dir = jobname\n",
        "log_filename = os.path.join(jobname,\"log.txt\")\n",
        "setup_logging(Path(log_filename))\n",
        "\n",
        "queries, is_complex = get_queries(queries_path)\n",
        "model_type = set_model_type(is_complex, model_type)\n",
        "\n",
        "if \"multimer\" in model_type and max_msa is not None:\n",
        "  use_cluster_profile = False\n",
        "else:\n",
        "  use_cluster_profile = True\n",
        "\n",
        "download_alphafold_params(model_type, Path(\".\"))\n",
        "results = run(\n",
        "    queries=queries,\n",
        "    result_dir=result_dir,\n",
        "    use_templates=use_templates,\n",
        "    custom_template_path=custom_template_path,\n",
        "    num_relax=num_relax,\n",
        "    msa_mode=msa_mode,\n",
        "    model_type=model_type,\n",
        "    num_models=5,\n",
        "    num_recycles=num_recycles,\n",
        "    relax_max_iterations=relax_max_iterations,\n",
        "    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
        "    num_seeds=num_seeds,\n",
        "    use_dropout=use_dropout,\n",
        "    model_order=[1,2,3,4,5],\n",
        "    is_complex=is_complex,\n",
        "    data_dir=Path(\".\"),\n",
        "    keep_existing_results=False,\n",
        "    rank_by=\"auto\",\n",
        "    pair_mode=pair_mode,\n",
        "    pairing_strategy=pairing_strategy,\n",
        "    stop_at_score=float(100),\n",
        "    prediction_callback=prediction_callback,\n",
        "    dpi=dpi,\n",
        "    zip_results=False,\n",
        "    save_all=save_all,\n",
        "    max_msa=max_msa,\n",
        "    use_cluster_profile=use_cluster_profile,\n",
        "    input_features_callback=input_features_callback,\n",
        "    save_recycles=save_recycles,\n",
        "    user_agent=\"colabfold/google-colab-main\",\n",
        ")\n",
        "results_zip = f\"{jobname}.result.zip\"\n",
        "os.system(f\"zip -r {results_zip} {jobname}\")"
      ],
      "metadata": {
        "id": "HnUFBb27B-Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display 3D structure {run: \"auto\"}\n",
        "import py3Dmol\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from colabfold.colabfold import plot_plddt_legend\n",
        "from colabfold.colabfold import pymol_color_list, alphabet_list\n",
        "rank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "\n",
        "tag = results[\"rank\"][0][rank_num - 1]\n",
        "jobname_prefix = \".custom\" if msa_mode == \"custom\" else \"\"\n",
        "pdb_filename = f\"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb\"\n",
        "pdb_file = glob.glob(pdb_filename)\n",
        "\n",
        "def show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
        "  model_name = f\"rank_{rank_num}\"\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(pdb_file[0],'r').read(),'pdb')\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    chains = len(queries[0][1]) + 1 if is_complex else 1\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "show_pdb(rank_num, show_sidechains, show_mainchains, color).show()\n",
        "if color == \"lDDT\":\n",
        "  plot_plddt_legend().show()"
      ],
      "metadata": {
        "id": "DUyVJxjJB_oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plots {run: \"auto\"}\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "from html import escape\n",
        "\n",
        "# see: https://stackoverflow.com/a/53688522\n",
        "def image_to_data_url(filename):\n",
        "  ext = filename.split('.')[-1]\n",
        "  prefix = f'data:image/{ext};base64,'\n",
        "  with open(filename, 'rb') as f:\n",
        "    img = f.read()\n",
        "  return prefix + base64.b64encode(img).decode('utf-8')\n",
        "\n",
        "pae = \"\"\n",
        "pae_file = os.path.join(jobname,f\"{jobname}{jobname_prefix}_pae.png\")\n",
        "if os.path.isfile(pae_file):\n",
        "    pae = image_to_data_url(pae_file)\n",
        "cov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\n",
        "plddt = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\n",
        "display(HTML(f\"\"\"\n",
        "<style>\n",
        "  img {{\n",
        "    float:left;\n",
        "  }}\n",
        "  .full {{\n",
        "    max-width:100%;\n",
        "  }}\n",
        "  .half {{\n",
        "    max-width:50%;\n",
        "  }}\n",
        "  @media (max-width:640px) {{\n",
        "    .half {{\n",
        "      max-width:100%;\n",
        "    }}\n",
        "  }}\n",
        "</style>\n",
        "<div style=\"max-width:90%; padding:2em;\">\n",
        "  <h1>Plots for {escape(jobname)}</h1>\n",
        "  { '<!--' if pae == '' else '' }<img src=\"{pae}\" class=\"full\" />{ '-->' if pae == '' else '' }\n",
        "  <img src=\"{cov}\" class=\"half\" />\n",
        "  <img src=\"{plddt}\" class=\"half\" />\n",
        "</div>\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "DRmE2-pwCHkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Package and download results\n",
        "#@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "if msa_mode == \"custom\":\n",
        "  print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "\n",
        "files.download(f\"{jobname}.result.zip\")\n",
        "\n",
        "if save_to_google_drive == True and drive:\n",
        "  uploaded = drive.CreateFile({'title': f\"{jobname}.result.zip\"})\n",
        "  uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
        "  uploaded.Upload()\n",
        "  print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
      ],
      "metadata": {
        "id": "k6WxZtyACXr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions <a name=\"Instructions\"></a>\n",
        "For detailed instructions, tips and tricks, see recently published paper at [Nature Protocols](https://www.nature.com/articles/s41596-024-01060-5)\n",
        "\n",
        "**Quick start**\n",
        "1. Paste your protein sequence(s) in the input field.\n",
        "2. Press \"Runtime\" -> \"Run all\".\n",
        "3. The pipeline consists of 5 steps. The currently running step is indicated by a circle with a stop sign next to it.\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "1. PDB formatted structures sorted by avg. pLDDT and complexes are sorted by pTMscore. (unrelaxed and relaxed if `use_amber` is enabled).\n",
        "2. Plots of the model quality.\n",
        "3. Plots of the MSA coverage.\n",
        "4. Parameter log file.\n",
        "5. A3M formatted input MSA.\n",
        "6. A `predicted_aligned_error_v1.json` using [AlphaFold-DB's format](https://alphafold.ebi.ac.uk/faq#faq-7) and a `scores.json` for each model which contains an array (list of lists) for PAE, a list with the average pLDDT and the pTMscore.\n",
        "7. BibTeX file with citations for all used tools and databases.\n",
        "\n",
        "At the end of the job a download modal box will pop up with a `jobname.result.zip` file. Additionally, if the `save_to_google_drive` option was selected, the `jobname.result.zip` will be uploaded to your Google Drive.\n",
        "\n",
        "**MSA generation for complexes**\n",
        "\n",
        "For the complex prediction we use unpaired and paired MSAs. Unpaired MSA is generated the same way as for the protein structures prediction by searching the UniRef100 and environmental sequences three iterations each.\n",
        "\n",
        "The paired MSA is generated by searching the UniRef100 database and pairing the best hits sharing the same NCBI taxonomic identifier (=species or sub-species). We only pair sequences if all of the query sequences are present for the respective taxonomic identifier.\n",
        "\n",
        "**Using a custom MSA as input**\n",
        "\n",
        "To predict the structure with a custom MSA (A3M formatted): (1) Change the `msa_mode`: to \"custom\", (2) Wait for an upload box to appear at the end of the \"MSA options ...\" box. Upload your A3M. The first fasta entry of the A3M must be the query sequence without gaps.\n",
        "\n",
        "It is also possilbe to proide custom MSAs for complex predictions. Read more about the format [here](https://github.com/sokrypton/ColabFold/issues/76).\n",
        "\n",
        "As an alternative for MSA generation the [HHblits Toolkit server](https://toolkit.tuebingen.mpg.de/tools/hhblits) can be used. After submitting your query, click \"Query Template MSA\" -> \"Download Full A3M\". Download the A3M file and upload it in this notebook.\n",
        "\n",
        "**PDB100** <a name=\"pdb100\"></a>\n",
        "\n",
        "As of 23/06/08, we have transitioned from using the PDB70 to a 100% clustered PDB, the PDB100. The construction methodology of PDB100 differs from that of PDB70.\n",
        "\n",
        "The PDB70 was constructed by running each PDB70 representative sequence through [HHblits](https://github.com/soedinglab/hh-suite) against the [Uniclust30](https://uniclust.mmseqs.com/). On the other hand, the PDB100 is built by searching each PDB100 representative structure with [Foldseek](https://github.com/steineggerlab/foldseek) against the [AlphaFold Database](https://alphafold.ebi.ac.uk).\n",
        "\n",
        "To maintain compatibility with older Notebook versions and local installations, the generated files and API responses will continue to be named \"PDB70\", even though we're now using the PDB100.\n",
        "\n",
        "**Using custom templates** <a name=\"custom_templates\"></a>\n",
        "\n",
        "To predict the structure with a custom template (PDB or mmCIF formatted): (1) change the `template_mode` to \"custom\" in the execute cell and (2) wait for an upload box to appear at the end of the \"Input Protein\" box. Select and upload your templates (multiple choices are possible).\n",
        "\n",
        "* Templates must follow the four letter PDB naming with lower case letters.\n",
        "\n",
        "* Templates in mmCIF format must contain `_entity_poly_seq`. An error is thrown if this field is not present. The field `_pdbx_audit_revision_history.revision_date` is automatically generated if it is not present.\n",
        "\n",
        "* Templates in PDB format are automatically converted to the mmCIF format. `_entity_poly_seq` and `_pdbx_audit_revision_history.revision_date` are automatically generated.\n",
        "\n",
        "If you encounter problems, please report them to this [issue](https://github.com/sokrypton/ColabFold/issues/177).\n",
        "\n",
        "**Comparison to the full AlphaFold2 and AlphaFold2 Colab**\n",
        "\n",
        "This notebook replaces the homology detection and MSA pairing of AlphaFold2 with MMseqs2. For a comparison against the [AlphaFold2 Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) and the full [AlphaFold2](https://github.com/deepmind/alphafold) system read our [paper](https://www.nature.com/articles/s41592-022-01488-1).\n",
        "\n",
        "**Troubleshooting**\n",
        "* Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "* Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "* Check your input sequence.\n",
        "\n",
        "**Known issues**\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
        "* Your browser can block the pop-up for downloading the result file. You can choose the `save_to_google_drive` option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "**Limitations**\n",
        "* Computing resources: Our MMseqs2 API can handle ~20-50k requests per day.\n",
        "* MSAs: MMseqs2 is very precise and sensitive but might find less hits compared to HHblits/HMMer searched against BFD or MGnify.\n",
        "* We recommend to additionally use the full [AlphaFold2 pipeline](https://github.com/deepmind/alphafold).\n",
        "\n",
        "**Description of the plots**\n",
        "*   **Number of sequences per position** - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
        "\n",
        "**Bugs**\n",
        "- If you encounter any bugs, please report the issue to https://github.com/sokrypton/ColabFold/issues\n",
        "\n",
        "**License**\n",
        "\n",
        "The source code of ColabFold is licensed under [MIT](https://raw.githubusercontent.com/sokrypton/ColabFold/main/LICENSE). Additionally, this notebook uses the AlphaFold2 source code and its parameters licensed under [Apache 2.0](https://raw.githubusercontent.com/deepmind/alphafold/main/LICENSE) and [CC BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/) respectively. Read more about the AlphaFold license [here](https://github.com/deepmind/alphafold).\n",
        "\n",
        "**Acknowledgments**\n",
        "- We thank the AlphaFold team for developing an excellent model and open sourcing the software.\n",
        "\n",
        "- [KOBIC](https://kobic.re.kr) and [Söding Lab](https://www.mpinat.mpg.de/soeding) for providing the computational resources for the MMseqs2 MSA server.\n",
        "\n",
        "- Richard Evans for helping to benchmark the ColabFold's Alphafold-multimer support.\n",
        "\n",
        "- [David Koes](https://github.com/dkoes) for his awesome [py3Dmol](https://3dmol.csb.pitt.edu/) plugin, without whom these notebooks would be quite boring!\n",
        "\n",
        "- Do-Yoon Kim for creating the ColabFold logo.\n",
        "\n",
        "- A colab by Sergey Ovchinnikov ([@sokrypton](https://twitter.com/sokrypton)), Milot Mirdita ([@milot_mirdita](https://twitter.com/milot_mirdita)) and Martin Steinegger ([@thesteinegger](https://twitter.com/thesteinegger)).\n"
      ],
      "metadata": {
        "id": "5HNJz9BUCelO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install system dependencies\n",
        "!apt-get update\n",
        "!apt-get install aria2\n",
        "!pip install -U pip\n",
        "!pip install -r https://raw.githubusercontent.com/deepmind/alphafold/main/docker/requirements.txt\n"
      ],
      "metadata": {
        "id": "UHs6bd-D6fne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/deepmind/alphafold.git\n",
        "!cd alphafold"
      ],
      "metadata": {
        "id": "oAJ6ut1w6ji2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to save downloaded databases and models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7V4AAvDf6mWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the databases to your Google Drive (takes time and bandwidth)\n",
        "!bash scripts/download_all_data.sh /content/drive/MyDrive/alphafold_data\n"
      ],
      "metadata": {
        "id": "NAhiK_c46pyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Docker image\n",
        "!docker build -f docker/Dockerfile -t alphafold .\n"
      ],
      "metadata": {
        "id": "MZKxAxKu6tuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example command to run AlphaFold in Colab\n",
        "!python3 docker/run_docker.py \\\n",
        "  --fasta_paths=/content/your_protein.fasta \\\n",
        "  --max_template_date=2022-01-01 \\\n",
        "  --data_dir=/content/drive/MyDrive/alphafold_data \\\n",
        "  --output_dir=/content/alphafold_output\n"
      ],
      "metadata": {
        "id": "LgqnVJEj6-dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc5-mbsX9PZC"
      },
      "source": [
        "# AlphaFold Colab\n",
        "\n",
        "This Colab notebook allows you to easily predict the structure of a protein using a slightly simplified version of [AlphaFold v2.3.2](https://doi.org/10.1038/s41586-021-03819-2).\n",
        "\n",
        "**Differences to AlphaFold v2.3.2**\n",
        "\n",
        "In comparison to AlphaFold v2.3.2, this Colab notebook uses **no templates (homologous structures)** and a selected portion of the [BFD database](https://bfd.mmseqs.com/). We have validated these changes on several thousand recent PDB structures. While accuracy will be near-identical to the full AlphaFold system on many targets, a small fraction have a large drop in accuracy due to the smaller MSA and lack of templates. For best reliability, we recommend instead using the [full open source AlphaFold](https://github.com/deepmind/alphafold/), or the [AlphaFold Protein Structure Database](https://alphafold.ebi.ac.uk/).\n",
        "\n",
        "**This Colab has a small drop in average accuracy for multimers compared to local AlphaFold installation, for full multimer accuracy it is highly recommended to run [AlphaFold locally](https://github.com/deepmind/alphafold#running-alphafold).** Moreover, the AlphaFold-Multimer requires searching for MSA for every unique sequence in the complex, hence it is substantially slower. If your notebook times-out due to slow multimer MSA search, we recommend either using Colab Pro or running AlphaFold locally.\n",
        "\n",
        "Please note that this Colab notebook is provided for theoretical modelling only and caution should be exercised in its use.\n",
        "\n",
        "The **PAE file format** has been updated to match AFDB. Please see the [AFDB FAQ](https://alphafold.ebi.ac.uk/faq/#faq-7) for a description of the new format.\n",
        "\n",
        "**Citing this work**\n",
        "\n",
        "Any publication that discloses findings arising from using this notebook should [cite](https://github.com/deepmind/alphafold/#citing-this-work) the [AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2).\n",
        "\n",
        "**Licenses**\n",
        "\n",
        "This Colab uses the [AlphaFold model parameters](https://github.com/deepmind/alphafold/#model-parameters-license) which are subject to the Creative Commons Attribution 4.0 International ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode)) license. The Colab itself is provided under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0). See the full license statement below.\n",
        "\n",
        "**More information**\n",
        "\n",
        "You can find more information about how AlphaFold works in the following papers:\n",
        "\n",
        "*   [AlphaFold methods paper](https://www.nature.com/articles/s41586-021-03819-2)\n",
        "*   [AlphaFold predictions of the human proteome paper](https://www.nature.com/articles/s41586-021-03828-1)\n",
        "*   [AlphaFold-Multimer paper](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1)\n",
        "\n",
        "FAQ on how to interpret AlphaFold predictions are [here](https://alphafold.ebi.ac.uk/faq).\n",
        "\n",
        "If you have any questions not covered in the FAQ, please contact the AlphaFold team at alphafold@deepmind.com.\n",
        "\n",
        "**Get in touch**\n",
        "\n",
        "We would love to hear your feedback and understand how AlphaFold has been useful in your research. Share your stories with us at alphafold@deepmind.com.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC1dKAwk2eyl"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Start by running the 2 cells below to set up AlphaFold and all required software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "woIxeCPygt7K"
      },
      "outputs": [],
      "source": [
        "# Set environment variables before running any other code.\n",
        "import os\n",
        "os.environ['TF_FORCE_UNIFIED_MEMORY'] = '1'\n",
        "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '4.0'\n",
        "\n",
        "#@title 1. Install third-party software\n",
        "\n",
        "#@markdown Please execute this cell by pressing the _Play_ button\n",
        "#@markdown on the left to download and import third-party software\n",
        "#@markdown in this Colab notebook. (See the [acknowledgements](https://github.com/deepmind/alphafold/#acknowledgements) in our readme.)\n",
        "\n",
        "#@markdown **Note**: This installs the software on the Colab\n",
        "#@markdown notebook in the cloud and not on your computer.\n",
        "\n",
        "from IPython.utils import io\n",
        "import os\n",
        "import subprocess\n",
        "import tqdm.notebook\n",
        "\n",
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "\n",
        "try:\n",
        "  with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      # Uninstall default Colab version of TF.\n",
        "      %shell pip uninstall -y tensorflow keras\n",
        "\n",
        "      %shell sudo apt install --quiet --yes hmmer\n",
        "      pbar.update(6)\n",
        "\n",
        "      # Install py3dmol.\n",
        "      %shell pip install py3dmol\n",
        "      pbar.update(2)\n",
        "\n",
        "      # Install OpenMM and pdbfixer.\n",
        "      %shell rm -rf /opt/conda\n",
        "      %shell wget -q -P /tmp \\\n",
        "        https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
        "          && bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n",
        "          && rm /tmp/Miniconda3-latest-Linux-x86_64.sh\n",
        "      pbar.update(9)\n",
        "\n",
        "      PATH=%env PATH\n",
        "      %env PATH=/opt/conda/bin:{PATH}\n",
        "      %shell conda install -qy conda==24.1.2 \\\n",
        "          && conda install -qy -c conda-forge \\\n",
        "            python=3.10 \\\n",
        "            openmm=8.0.0 \\\n",
        "            pdbfixer\n",
        "      pbar.update(80)\n",
        "\n",
        "      # Create a ramdisk to store a database chunk to make Jackhmmer run fast.\n",
        "      %shell sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "      %shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "      pbar.update(2)\n",
        "\n",
        "      %shell wget -q -P /content \\\n",
        "        https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "      pbar.update(1)\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "executed_cells = set([1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VzJ5iMjTtoZw"
      },
      "outputs": [],
      "source": [
        "#@title 2. Download AlphaFold\n",
        "\n",
        "#@markdown Please execute this cell by pressing the *Play* button on\n",
        "#@markdown the left.\n",
        "\n",
        "GIT_REPO = 'https://github.com/deepmind/alphafold'\n",
        "SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_colab_2022-12-06.tar'\n",
        "PARAMS_DIR = './alphafold/data/params'\n",
        "PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(SOURCE_URL))\n",
        "\n",
        "try:\n",
        "  with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      %shell rm -rf alphafold\n",
        "      %shell git clone --branch main {GIT_REPO} alphafold\n",
        "      pbar.update(8)\n",
        "      # Install the required versions of all dependencies.\n",
        "      %shell pip3 install -r ./alphafold/requirements.txt\n",
        "      # Run setup.py to install only AlphaFold.\n",
        "      %shell pip3 install --no-dependencies ./alphafold\n",
        "      %shell pip3 install pyopenssl==22.0.0\n",
        "      pbar.update(10)\n",
        "\n",
        "      # Make sure stereo_chemical_props.txt is in all locations where it could be searched for.\n",
        "      %shell mkdir -p /content/alphafold/alphafold/common\n",
        "      %shell cp -f /content/stereo_chemical_props.txt /content/alphafold/alphafold/common\n",
        "      %shell mkdir -p /opt/conda/lib/python3.10/site-packages/alphafold/common/\n",
        "      %shell cp -f /content/stereo_chemical_props.txt /opt/conda/lib/python3.10/site-packages/alphafold/common/\n",
        "\n",
        "      # Load parameters\n",
        "      %shell mkdir --parents \"{PARAMS_DIR}\"\n",
        "      %shell wget -O \"{PARAMS_PATH}\" \"{SOURCE_URL}\"\n",
        "      pbar.update(27)\n",
        "\n",
        "      %shell tar --extract --verbose --file=\"{PARAMS_PATH}\" \\\n",
        "        --directory=\"{PARAMS_DIR}\" --preserve-permissions\n",
        "      %shell rm \"{PARAMS_PATH}\"\n",
        "      pbar.update(55)\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "import jax\n",
        "if jax.local_devices()[0].platform == 'tpu':\n",
        "  raise RuntimeError('Colab TPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "elif jax.local_devices()[0].platform == 'cpu':\n",
        "  raise RuntimeError('Colab CPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "else:\n",
        "  print(f'Running with {jax.local_devices()[0].device_kind} GPU')\n",
        "\n",
        "# Make sure everything we need is on the path.\n",
        "import sys\n",
        "sys.path.append('/opt/conda/lib/python3.10/site-packages')\n",
        "sys.path.append('/content/alphafold')\n",
        "\n",
        "executed_cells.add(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4JpOs6oA-QS"
      },
      "source": [
        "## Making a prediction\n",
        "\n",
        "Please paste the sequence of your protein in the text box below, then run the remaining cells via _Runtime_ > _Run after_. You can also run the cells individually by pressing the _Play_ button on the left.\n",
        "\n",
        "Note that the search against databases and the actual prediction can take some time, from minutes to hours, depending on the length of the protein and what type of GPU you are allocated by Colab (see FAQ below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rowN0bVYLe9n"
      },
      "outputs": [],
      "source": [
        "#@title 3. Enter the amino acid sequence(s) to fold ⬇️\n",
        "#@markdown Enter the amino acid sequence(s) to fold:\n",
        "#@markdown * If you enter only a single sequence, the monomer model will be\n",
        "#@markdown used (unless you override this below).\n",
        "#@markdown * If you enter multiple sequences, the multimer model will be used.\n",
        "\n",
        "from alphafold.notebooks import notebook_utils\n",
        "# Track cell execution to ensure correct order.\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 3)\n",
        "\n",
        "import enum\n",
        "\n",
        "@enum.unique\n",
        "class ModelType(enum.Enum):\n",
        "  MONOMER = 0\n",
        "  MULTIMER = 1\n",
        "\n",
        "sequence_1 = 'MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH'  #@param {type:\"string\"}\n",
        "sequence_2 = ''  #@param {type:\"string\"}\n",
        "sequence_3 = ''  #@param {type:\"string\"}\n",
        "sequence_4 = ''  #@param {type:\"string\"}\n",
        "sequence_5 = ''  #@param {type:\"string\"}\n",
        "sequence_6 = ''  #@param {type:\"string\"}\n",
        "sequence_7 = ''  #@param {type:\"string\"}\n",
        "sequence_8 = ''  #@param {type:\"string\"}\n",
        "sequence_9 = ''  #@param {type:\"string\"}\n",
        "sequence_10 = ''  #@param {type:\"string\"}\n",
        "sequence_11 = ''  #@param {type:\"string\"}\n",
        "sequence_12 = ''  #@param {type:\"string\"}\n",
        "sequence_13 = ''  #@param {type:\"string\"}\n",
        "sequence_14 = ''  #@param {type:\"string\"}\n",
        "sequence_15 = ''  #@param {type:\"string\"}\n",
        "sequence_16 = ''  #@param {type:\"string\"}\n",
        "sequence_17 = ''  #@param {type:\"string\"}\n",
        "sequence_18 = ''  #@param {type:\"string\"}\n",
        "sequence_19 = ''  #@param {type:\"string\"}\n",
        "sequence_20 = ''  #@param {type:\"string\"}\n",
        "\n",
        "input_sequences = (\n",
        "    sequence_1, sequence_2, sequence_3, sequence_4, sequence_5,\n",
        "    sequence_6, sequence_7, sequence_8, sequence_9, sequence_10,\n",
        "    sequence_11, sequence_12, sequence_13, sequence_14, sequence_15,\n",
        "    sequence_16, sequence_17, sequence_18, sequence_19, sequence_20)\n",
        "\n",
        "MIN_PER_SEQUENCE_LENGTH = 16\n",
        "MAX_PER_SEQUENCE_LENGTH = 4000\n",
        "MAX_MONOMER_MODEL_LENGTH = 2500\n",
        "MAX_LENGTH = 4000\n",
        "MAX_VALIDATED_LENGTH = 3000\n",
        "\n",
        "#@markdown Select this checkbox to run the multimer model for a single sequence.\n",
        "#@markdown For proteins that are monomeric in their native form, or for very\n",
        "#@markdown large single chains you may get better accuracy and memory efficiency\n",
        "#@markdown by using the multimer model.\n",
        "#@markdown\n",
        "#@markdown\n",
        "#@markdown Due to improved memory efficiency the multimer model has a maximum\n",
        "#@markdown limit of 4000 residues, while the monomer model has a limit of 2500\n",
        "#@markdown residues.\n",
        "\n",
        "use_multimer_model_for_monomers = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Validate the input sequences.\n",
        "sequences = notebook_utils.clean_and_validate_input_sequences(\n",
        "    input_sequences=input_sequences,\n",
        "    min_sequence_length=MIN_PER_SEQUENCE_LENGTH,\n",
        "    max_sequence_length=MAX_PER_SEQUENCE_LENGTH)\n",
        "\n",
        "if len(sequences) == 1:\n",
        "  if use_multimer_model_for_monomers:\n",
        "    print('Using the multimer model for single-chain, as requested.')\n",
        "    model_type_to_use = ModelType.MULTIMER\n",
        "  else:\n",
        "    print('Using the single-chain model.')\n",
        "    model_type_to_use = ModelType.MONOMER\n",
        "else:\n",
        "  print(f'Using the multimer model with {len(sequences)} sequences.')\n",
        "  model_type_to_use = ModelType.MULTIMER\n",
        "\n",
        "# Check whether total length exceeds limit.\n",
        "total_sequence_length = sum([len(seq) for seq in sequences])\n",
        "if total_sequence_length > MAX_LENGTH:\n",
        "  raise ValueError('The total sequence length is too long: '\n",
        "                   f'{total_sequence_length}, while the maximum is '\n",
        "                   f'{MAX_LENGTH}.')\n",
        "\n",
        "# Check whether we exceed the monomer limit.\n",
        "if model_type_to_use == ModelType.MONOMER:\n",
        "  if len(sequences[0]) > MAX_MONOMER_MODEL_LENGTH:\n",
        "    raise ValueError(\n",
        "        f'Input sequence is too long: {len(sequences[0])} amino acids, while '\n",
        "        f'the maximum for the monomer model is {MAX_MONOMER_MODEL_LENGTH}. You may '\n",
        "        'be able to run this sequence with the multimer model by selecting the '\n",
        "        'use_multimer_model_for_monomers checkbox above.')\n",
        "\n",
        "if total_sequence_length > MAX_VALIDATED_LENGTH:\n",
        "  print('WARNING: The accuracy of the system has not been fully validated '\n",
        "        'above 3000 residues, and you may experience long running times or '\n",
        "        f'run out of memory. Total sequence length is {total_sequence_length} '\n",
        "        'residues.')\n",
        "\n",
        "executed_cells.add(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2tTeTTsLKPjB"
      },
      "outputs": [],
      "source": [
        "#@title 4. Search against genetic databases\n",
        "\n",
        "#@markdown Once this cell has been executed, you will see\n",
        "#@markdown statistics about the multiple sequence alignment\n",
        "#@markdown (MSA) that will be used by AlphaFold. In particular,\n",
        "#@markdown you’ll see how well each residue is covered by similar\n",
        "#@markdown sequences in the MSA.\n",
        "\n",
        "# Track cell execution to ensure correct order\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 4)\n",
        "\n",
        "# --- Python imports ---\n",
        "import collections\n",
        "import copy\n",
        "from concurrent import futures\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from urllib import request\n",
        "from google.colab import files\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "\n",
        "from alphafold.model import model\n",
        "from alphafold.model import config\n",
        "from alphafold.model import data\n",
        "\n",
        "from alphafold.data import feature_processing\n",
        "from alphafold.data import msa_pairing\n",
        "from alphafold.data import pipeline\n",
        "from alphafold.data import pipeline_multimer\n",
        "from alphafold.data.tools import jackhmmer\n",
        "\n",
        "from alphafold.common import confidence\n",
        "from alphafold.common import protein\n",
        "\n",
        "from alphafold.relax import relax\n",
        "from alphafold.relax import utils\n",
        "\n",
        "from IPython import display\n",
        "from ipywidgets import GridspecLayout\n",
        "from ipywidgets import Output\n",
        "\n",
        "# Color bands for visualizing plddt\n",
        "PLDDT_BANDS = [(0, 50, '#FF7D45'),\n",
        "               (50, 70, '#FFDB13'),\n",
        "               (70, 90, '#65CBF3'),\n",
        "               (90, 100, '#0053D6')]\n",
        "\n",
        "# --- Find the closest source ---\n",
        "test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2022_01.fasta.1'\n",
        "ex = futures.ThreadPoolExecutor(3)\n",
        "def fetch(source):\n",
        "  request.urlretrieve(test_url_pattern.format(source))\n",
        "  return source\n",
        "fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
        "source = None\n",
        "for f in futures.as_completed(fs):\n",
        "  source = f.result()\n",
        "  ex.shutdown()\n",
        "  break\n",
        "\n",
        "JACKHMMER_BINARY_PATH = '/usr/bin/jackhmmer'\n",
        "DB_ROOT_PATH = f'https://storage.googleapis.com/alphafold-colab{source}/latest/'\n",
        "# The z_value is the number of sequences in a database.\n",
        "MSA_DATABASES = [\n",
        "    {'db_name': 'uniref90',\n",
        "     'db_path': f'{DB_ROOT_PATH}uniref90_2022_01.fasta',\n",
        "     'num_streamed_chunks': 62,\n",
        "     'z_value': 144_113_457},\n",
        "    {'db_name': 'smallbfd',\n",
        "     'db_path': f'{DB_ROOT_PATH}bfd-first_non_consensus_sequences.fasta',\n",
        "     'num_streamed_chunks': 17,\n",
        "     'z_value': 65_984_053},\n",
        "    {'db_name': 'mgnify',\n",
        "     'db_path': f'{DB_ROOT_PATH}mgy_clusters_2022_05.fasta',\n",
        "     'num_streamed_chunks': 120,\n",
        "     'z_value': 623_796_864},\n",
        "]\n",
        "\n",
        "# Search UniProt and construct the all_seq features only for heteromers, not homomers.\n",
        "if model_type_to_use == ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "  MSA_DATABASES.extend([\n",
        "      # Swiss-Prot and TrEMBL are concatenated together as UniProt.\n",
        "      {'db_name': 'uniprot',\n",
        "       'db_path': f'{DB_ROOT_PATH}uniprot_2021_04.fasta',\n",
        "       'num_streamed_chunks': 101,\n",
        "       'z_value': 225_013_025 + 565_928},\n",
        "  ])\n",
        "\n",
        "TOTAL_JACKHMMER_CHUNKS = sum([cfg['num_streamed_chunks'] for cfg in MSA_DATABASES])\n",
        "\n",
        "MAX_HITS = {\n",
        "    'uniref90': 10_000,\n",
        "    'smallbfd': 5_000,\n",
        "    'mgnify': 501,\n",
        "    'uniprot': 50_000,\n",
        "}\n",
        "\n",
        "\n",
        "def get_msa(sequences):\n",
        "  \"\"\"Searches for MSA for given sequences using chunked Jackhmmer search.\n",
        "\n",
        "  Args:\n",
        "    sequences: A list of sequences to search against all databases.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary mapping unique sequences to dicionaries mapping each database\n",
        "    to a list of  results, one for each chunk of the database.\n",
        "  \"\"\"\n",
        "  sequence_to_fasta_path = {}\n",
        "  # Deduplicate to not do redundant work for multiple copies of the same chain in homomers.\n",
        "  for sequence_index, sequence in enumerate(sorted(set(sequences)), 1):\n",
        "    fasta_path = f'target_{sequence_index:02d}.fasta'\n",
        "    with open(fasta_path, 'wt') as f:\n",
        "      f.write(f'>query\\n{sequence}')\n",
        "    sequence_to_fasta_path[sequence] = fasta_path\n",
        "\n",
        "  # Run the search against chunks of genetic databases (since the genetic\n",
        "  # databases don't fit in Colab disk).\n",
        "  raw_msa_results = {sequence: {} for sequence in sequence_to_fasta_path.keys()}\n",
        "  print('\\nGetting MSA for all sequences')\n",
        "  with tqdm.notebook.tqdm(total=TOTAL_JACKHMMER_CHUNKS, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    def jackhmmer_chunk_callback(i):\n",
        "      pbar.update(n=1)\n",
        "\n",
        "    for db_config in MSA_DATABASES:\n",
        "      db_name = db_config['db_name']\n",
        "      pbar.set_description(f'Searching {db_name}')\n",
        "      jackhmmer_runner = jackhmmer.Jackhmmer(\n",
        "          binary_path=JACKHMMER_BINARY_PATH,\n",
        "          database_path=db_config['db_path'],\n",
        "          get_tblout=True,\n",
        "          num_streamed_chunks=db_config['num_streamed_chunks'],\n",
        "          streaming_callback=jackhmmer_chunk_callback,\n",
        "          z_value=db_config['z_value'])\n",
        "      # Query all unique sequences against each chunk of the database to prevent\n",
        "      # redunantly fetching each chunk for each unique sequence.\n",
        "      results = jackhmmer_runner.query_multiple(list(sequence_to_fasta_path.values()))\n",
        "      for sequence, result_for_sequence in zip(sequence_to_fasta_path.keys(), results):\n",
        "        raw_msa_results[sequence][db_name] = result_for_sequence\n",
        "\n",
        "  return raw_msa_results\n",
        "\n",
        "\n",
        "features_for_chain = {}\n",
        "raw_msa_results_for_sequence = get_msa(sequences)\n",
        "for sequence_index, sequence in enumerate(sequences, start=1):\n",
        "  raw_msa_results = copy.deepcopy(raw_msa_results_for_sequence[sequence])\n",
        "\n",
        "  # Extract the MSAs from the Stockholm files.\n",
        "  # NB: deduplication happens later in pipeline.make_msa_features.\n",
        "  single_chain_msas = []\n",
        "  uniprot_msa = None\n",
        "  for db_name, db_results in raw_msa_results.items():\n",
        "    merged_msa = notebook_utils.merge_chunked_msa(\n",
        "        results=db_results, max_hits=MAX_HITS.get(db_name))\n",
        "    if merged_msa.sequences and db_name != 'uniprot':\n",
        "      single_chain_msas.append(merged_msa)\n",
        "      msa_size = len(set(merged_msa.sequences))\n",
        "      print(f'{msa_size} unique sequences found in {db_name} for sequence {sequence_index}')\n",
        "    elif merged_msa.sequences and db_name == 'uniprot':\n",
        "      uniprot_msa = merged_msa\n",
        "\n",
        "  notebook_utils.show_msa_info(single_chain_msas=single_chain_msas, sequence_index=sequence_index)\n",
        "\n",
        "  # Turn the raw data into model features.\n",
        "  feature_dict = {}\n",
        "  feature_dict.update(pipeline.make_sequence_features(\n",
        "      sequence=sequence, description='query', num_res=len(sequence)))\n",
        "  feature_dict.update(pipeline.make_msa_features(msas=single_chain_msas))\n",
        "  # We don't use templates in AlphaFold Colab notebook, add only empty placeholder features.\n",
        "  feature_dict.update(notebook_utils.empty_placeholder_template_features(\n",
        "      num_templates=0, num_res=len(sequence)))\n",
        "\n",
        "  # Construct the all_seq features only for heteromers, not homomers.\n",
        "  if model_type_to_use == ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "    valid_feats = msa_pairing.MSA_FEATURES + (\n",
        "        'msa_species_identifiers',\n",
        "    )\n",
        "    all_seq_features = {\n",
        "        f'{k}_all_seq': v for k, v in pipeline.make_msa_features([uniprot_msa]).items()\n",
        "        if k in valid_feats}\n",
        "    feature_dict.update(all_seq_features)\n",
        "\n",
        "  features_for_chain[protein.PDB_CHAIN_IDS[sequence_index - 1]] = feature_dict\n",
        "\n",
        "\n",
        "# Do further feature post-processing depending on the model type.\n",
        "if model_type_to_use == ModelType.MONOMER:\n",
        "  np_example = features_for_chain[protein.PDB_CHAIN_IDS[0]]\n",
        "\n",
        "elif model_type_to_use == ModelType.MULTIMER:\n",
        "  all_chain_features = {}\n",
        "  for chain_id, chain_features in features_for_chain.items():\n",
        "    all_chain_features[chain_id] = pipeline_multimer.convert_monomer_features(\n",
        "        chain_features, chain_id)\n",
        "\n",
        "  all_chain_features = pipeline_multimer.add_assembly_features(all_chain_features)\n",
        "\n",
        "  np_example = feature_processing.pair_and_merge(\n",
        "      all_chain_features=all_chain_features)\n",
        "\n",
        "  # Pad MSA to avoid zero-sized extra_msa.\n",
        "  np_example = pipeline_multimer.pad_msa(np_example, min_num_seq=512)\n",
        "\n",
        "executed_cells.add(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XUo6foMQxwS2"
      },
      "outputs": [],
      "source": [
        "#@title 5. Run AlphaFold and download prediction\n",
        "\n",
        "#@markdown Once this cell has been executed, a zip-archive with\n",
        "#@markdown the obtained prediction will be automatically downloaded\n",
        "#@markdown to your computer.\n",
        "\n",
        "#@markdown In case you are having issues with the relaxation stage, you can disable it below.\n",
        "#@markdown Warning: This means that the prediction might have distracting\n",
        "#@markdown small stereochemical violations.\n",
        "\n",
        "run_relax = True  #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Relaxation is faster with a GPU, but we have found it to be less stable.\n",
        "#@markdown You may wish to enable GPU for higher performance, but if it doesn't\n",
        "#@markdown converge we suggested reverting to using without GPU.\n",
        "\n",
        "relax_use_gpu = False  #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown The multimer model will continue recycling until the predictions stop\n",
        "#@markdown changing, up to the limit set here. For higher accuracy, at the\n",
        "#@markdown potential cost of longer inference times, set this to 20.\n",
        "\n",
        "multimer_model_max_num_recycles = 3  #@param {type:\"integer\"}\n",
        "\n",
        "# Track cell execution to ensure correct order\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 5)\n",
        "\n",
        "# --- Run the model ---\n",
        "if model_type_to_use == ModelType.MONOMER:\n",
        "  model_names = config.MODEL_PRESETS['monomer'] + ('model_2_ptm',)\n",
        "elif model_type_to_use == ModelType.MULTIMER:\n",
        "  model_names = config.MODEL_PRESETS['multimer']\n",
        "\n",
        "output_dir = 'prediction'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "plddts = {}\n",
        "ranking_confidences = {}\n",
        "pae_outputs = {}\n",
        "unrelaxed_proteins = {}\n",
        "\n",
        "with tqdm.notebook.tqdm(total=len(model_names) + 1, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "  for model_name in model_names:\n",
        "    pbar.set_description(f'Running {model_name}')\n",
        "\n",
        "    cfg = config.model_config(model_name)\n",
        "\n",
        "    if model_type_to_use == ModelType.MONOMER:\n",
        "      cfg.data.eval.num_ensemble = 1\n",
        "    elif model_type_to_use == ModelType.MULTIMER:\n",
        "      cfg.model.num_ensemble_eval = 1\n",
        "\n",
        "    if model_type_to_use == ModelType.MULTIMER:\n",
        "      cfg.model.num_recycle = multimer_model_max_num_recycles\n",
        "      cfg.model.recycle_early_stop_tolerance = 0.5\n",
        "\n",
        "    params = data.get_model_haiku_params(model_name, './alphafold/data')\n",
        "    model_runner = model.RunModel(cfg, params)\n",
        "    processed_feature_dict = model_runner.process_features(np_example, random_seed=0)\n",
        "    prediction = model_runner.predict(processed_feature_dict, random_seed=random.randrange(sys.maxsize))\n",
        "\n",
        "    mean_plddt = prediction['plddt'].mean()\n",
        "\n",
        "    if model_type_to_use == ModelType.MONOMER:\n",
        "      if 'predicted_aligned_error' in prediction:\n",
        "        pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                   prediction['max_predicted_aligned_error'])\n",
        "      else:\n",
        "        # Monomer models are sorted by mean pLDDT. Do not put monomer pTM models here as they\n",
        "        # should never get selected.\n",
        "        ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "        plddts[model_name] = prediction['plddt']\n",
        "    elif model_type_to_use == ModelType.MULTIMER:\n",
        "      # Multimer models are sorted by pTM+ipTM.\n",
        "      ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "      plddts[model_name] = prediction['plddt']\n",
        "      pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                 prediction['max_predicted_aligned_error'])\n",
        "\n",
        "    # Set the b-factors to the per-residue plddt.\n",
        "    final_atom_mask = prediction['structure_module']['final_atom_mask']\n",
        "    b_factors = prediction['plddt'][:, None] * final_atom_mask\n",
        "    unrelaxed_protein = protein.from_prediction(\n",
        "        processed_feature_dict,\n",
        "        prediction,\n",
        "        b_factors=b_factors,\n",
        "        remove_leading_feature_dimension=(\n",
        "            model_type_to_use == ModelType.MONOMER))\n",
        "    unrelaxed_proteins[model_name] = unrelaxed_protein\n",
        "\n",
        "    # Delete unused outputs to save memory.\n",
        "    del model_runner\n",
        "    del params\n",
        "    del prediction\n",
        "    pbar.update(n=1)\n",
        "\n",
        "  # --- AMBER relax the best model ---\n",
        "\n",
        "  # Find the best model according to the mean pLDDT.\n",
        "  best_model_name = max(ranking_confidences.keys(), key=lambda x: ranking_confidences[x])\n",
        "\n",
        "  if run_relax:\n",
        "    pbar.set_description(f'AMBER relaxation')\n",
        "    amber_relaxer = relax.AmberRelaxation(\n",
        "        max_iterations=0,\n",
        "        tolerance=2.39,\n",
        "        stiffness=10.0,\n",
        "        exclude_residues=[],\n",
        "        max_outer_iterations=3,\n",
        "        use_gpu=relax_use_gpu)\n",
        "    relaxed_pdb, _, _ = amber_relaxer.process(prot=unrelaxed_proteins[best_model_name])\n",
        "  else:\n",
        "    print('Warning: Running without the relaxation stage.')\n",
        "    relaxed_pdb = protein.to_pdb(unrelaxed_proteins[best_model_name])\n",
        "  pbar.update(n=1)  # Finished AMBER relax.\n",
        "\n",
        "# Construct multiclass b-factors to indicate confidence bands\n",
        "# 0=very low, 1=low, 2=confident, 3=very high\n",
        "banded_b_factors = []\n",
        "for plddt in plddts[best_model_name]:\n",
        "  for idx, (min_val, max_val, _) in enumerate(PLDDT_BANDS):\n",
        "    if plddt >= min_val and plddt <= max_val:\n",
        "      banded_b_factors.append(idx)\n",
        "      break\n",
        "banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask\n",
        "to_visualize_pdb = utils.overwrite_b_factors(relaxed_pdb, banded_b_factors)\n",
        "\n",
        "\n",
        "# Write out the prediction\n",
        "pred_output_path = os.path.join(output_dir, 'selected_prediction.pdb')\n",
        "with open(pred_output_path, 'w') as f:\n",
        "  f.write(relaxed_pdb)\n",
        "\n",
        "\n",
        "# --- Visualise the prediction & confidence ---\n",
        "show_sidechains = True\n",
        "def plot_plddt_legend():\n",
        "  \"\"\"Plots the legend for pLDDT.\"\"\"\n",
        "  thresh = ['Very low (pLDDT < 50)',\n",
        "            'Low (70 > pLDDT > 50)',\n",
        "            'Confident (90 > pLDDT > 70)',\n",
        "            'Very high (pLDDT > 90)']\n",
        "\n",
        "  colors = [x[2] for x in PLDDT_BANDS]\n",
        "\n",
        "  plt.figure(figsize=(2, 2))\n",
        "  for c in colors:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False, loc='center', fontsize=20)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  ax = plt.gca()\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.spines['left'].set_visible(False)\n",
        "  ax.spines['bottom'].set_visible(False)\n",
        "  plt.title('Model Confidence', fontsize=20, pad=20)\n",
        "  return plt\n",
        "\n",
        "# Show the structure coloured by chain if the multimer model has been used.\n",
        "if model_type_to_use == ModelType.MULTIMER:\n",
        "  multichain_view = py3Dmol.view(width=800, height=600)\n",
        "  multichain_view.addModelsAsFrames(to_visualize_pdb)\n",
        "  multichain_style = {'cartoon': {'colorscheme': 'chain'}}\n",
        "  multichain_view.setStyle({'model': -1}, multichain_style)\n",
        "  multichain_view.zoomTo()\n",
        "  multichain_view.show()\n",
        "\n",
        "# Color the structure by per-residue pLDDT\n",
        "color_map = {i: bands[2] for i, bands in enumerate(PLDDT_BANDS)}\n",
        "view = py3Dmol.view(width=800, height=600)\n",
        "view.addModelsAsFrames(to_visualize_pdb)\n",
        "style = {'cartoon': {'colorscheme': {'prop': 'b', 'map': color_map}}}\n",
        "if show_sidechains:\n",
        "  style['stick'] = {}\n",
        "view.setStyle({'model': -1}, style)\n",
        "view.zoomTo()\n",
        "\n",
        "grid = GridspecLayout(1, 2)\n",
        "out = Output()\n",
        "with out:\n",
        "  view.show()\n",
        "grid[0, 0] = out\n",
        "\n",
        "out = Output()\n",
        "with out:\n",
        "  plot_plddt_legend().show()\n",
        "grid[0, 1] = out\n",
        "\n",
        "display.display(grid)\n",
        "\n",
        "# Display pLDDT and predicted aligned error (if output by the model).\n",
        "if pae_outputs:\n",
        "  num_plots = 2\n",
        "else:\n",
        "  num_plots = 1\n",
        "\n",
        "plt.figure(figsize=[8 * num_plots, 6])\n",
        "plt.subplot(1, num_plots, 1)\n",
        "plt.plot(plddts[best_model_name])\n",
        "plt.title('Predicted LDDT')\n",
        "plt.xlabel('Residue')\n",
        "plt.ylabel('pLDDT')\n",
        "\n",
        "if num_plots == 2:\n",
        "  plt.subplot(1, 2, 2)\n",
        "  pae, max_pae = list(pae_outputs.values())[0]\n",
        "  plt.imshow(pae, vmin=0., vmax=max_pae, cmap='Greens_r')\n",
        "  plt.colorbar(fraction=0.046, pad=0.04)\n",
        "\n",
        "  # Display lines at chain boundaries.\n",
        "  best_unrelaxed_prot = unrelaxed_proteins[best_model_name]\n",
        "  total_num_res = best_unrelaxed_prot.residue_index.shape[-1]\n",
        "  chain_ids = best_unrelaxed_prot.chain_index\n",
        "  for chain_boundary in np.nonzero(chain_ids[:-1] - chain_ids[1:]):\n",
        "    if chain_boundary.size:\n",
        "      plt.plot([0, total_num_res], [chain_boundary, chain_boundary], color='red')\n",
        "      plt.plot([chain_boundary, chain_boundary], [0, total_num_res], color='red')\n",
        "\n",
        "  plt.title('Predicted Aligned Error')\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "\n",
        "# Save the predicted aligned error (if it exists).\n",
        "pae_output_path = os.path.join(output_dir, 'predicted_aligned_error.json')\n",
        "if pae_outputs:\n",
        "  # Save predicted aligned error in the same format as the AF EMBL DB.\n",
        "  pae_data = confidence.pae_json(pae=pae, max_pae=max_pae.item())\n",
        "  with open(pae_output_path, 'w') as f:\n",
        "    f.write(pae_data)\n",
        "\n",
        "# --- Download the predictions ---\n",
        "shutil.make_archive(base_name='prediction', format='zip', root_dir=output_dir)\n",
        "files.download(f'{output_dir}.zip')\n",
        "\n",
        "executed_cells.add(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUQAn5LYC5n4"
      },
      "source": [
        "### Interpreting the prediction\n",
        "\n",
        "In general predicted LDDT (pLDDT) is best used for intra-domain confidence, whereas Predicted Aligned Error (PAE) is best used for determining between domain or between chain confidence.\n",
        "\n",
        "Please see the [AlphaFold methods paper](https://www.nature.com/articles/s41586-021-03819-2), the [AlphaFold predictions of the human proteome paper](https://www.nature.com/articles/s41586-021-03828-1), and the [AlphaFold-Multimer paper](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1) as well as [our FAQ](https://alphafold.ebi.ac.uk/faq) on how to interpret AlphaFold predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeb2z8DIA4om"
      },
      "source": [
        "## FAQ & Troubleshooting\n",
        "\n",
        "\n",
        "*   How do I get a predicted protein structure for my protein?\n",
        "    *   Click on the _Connect_ button on the top right to get started.\n",
        "    *   Paste the amino acid sequence of your protein (without any headers) into the “Enter the amino acid sequence to fold”.\n",
        "    *   Run all cells in the Colab, either by running them individually (with the play button on the left side) or via _Runtime_ > _Run all._ Make sure you run all 5 cells in order.\n",
        "    *   The predicted protein structure will be downloaded once all cells have been executed. Note: This can take minutes to hours - see below.\n",
        "*   How long will this take?\n",
        "    *   Downloading the AlphaFold source code can take up to a few minutes.\n",
        "    *   Downloading and installing the third-party software can take up to a few minutes.\n",
        "    *   The search against genetic databases can take minutes to hours.\n",
        "    *   Running AlphaFold and generating the prediction can take minutes to hours, depending on the length of your protein and on which GPU-type Colab has assigned you.\n",
        "*   My Colab no longer seems to be doing anything, what should I do?\n",
        "    *   Some steps may take minutes to hours to complete.\n",
        "    *   If nothing happens or if you receive an error message, try restarting your Colab runtime via _Runtime_ > _Restart runtime_.\n",
        "    *   If this doesn’t help, try resetting your Colab runtime via _Runtime_ > _Factory reset runtime_.\n",
        "*   How does this compare to the open-source version of AlphaFold?\n",
        "    *   This Colab version of AlphaFold searches a selected portion of the BFD dataset and currently doesn’t use templates, so its accuracy is reduced in comparison to the full version of AlphaFold that is described in the [AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2) and [Github repo](https://github.com/deepmind/alphafold/) (the full version is available via the inference script).\n",
        "*   What is a Colab?\n",
        "    *   See the [Colab FAQ](https://research.google.com/colaboratory/faq.html).\n",
        "*   I received a warning “Notebook requires high RAM”, what do I do?\n",
        "    *   The resources allocated to your Colab vary. See the [Colab FAQ](https://research.google.com/colaboratory/faq.html) for more details.\n",
        "    *   You can execute the Colab nonetheless.\n",
        "*   I received an error “Colab CPU runtime not supported” or “No GPU/TPU found”, what do I do?\n",
        "    *   Colab CPU runtime is not supported. Try changing your runtime via _Runtime_ > _Change runtime type_ > _Hardware accelerator_ > _GPU_.\n",
        "    *   The type of GPU allocated to your Colab varies. See the [Colab FAQ](https://research.google.com/colaboratory/faq.html) for more details.\n",
        "    *   If you receive “Cannot connect to GPU backend”, you can try again later to see if Colab allocates you a GPU.\n",
        "    *   [Colab Pro](https://colab.research.google.com/signup) offers priority access to GPUs.\n",
        "*   I received an error “ModuleNotFoundError: No module named ...”, even though I ran the cell that imports it, what do I do?\n",
        "    *   Colab notebooks on the free tier time out after a certain amount of time. See the [Colab FAQ](https://research.google.com/colaboratory/faq.html#idle-timeouts). Try rerunning the whole notebook from the beginning.\n",
        "*   Does this tool install anything on my computer?\n",
        "    *   No, everything happens in the cloud on Google Colab.\n",
        "    *   At the end of the Colab execution a zip-archive with the obtained prediction will be automatically downloaded to your computer.\n",
        "*   How should I share feedback and bug reports?\n",
        "    *   Please share any feedback and bug reports as an [issue](https://github.com/deepmind/alphafold/issues) on Github.\n",
        "\n",
        "\n",
        "## Related work\n",
        "\n",
        "Take a look at these Colab notebooks provided by the community (please note that these notebooks may vary from our validated AlphaFold system and we cannot guarantee their accuracy):\n",
        "\n",
        "*   The [ColabFold AlphaFold2 notebook](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb) by Sergey Ovchinnikov, Milot Mirdita and Martin Steinegger, which uses an API hosted at the Södinglab based on the MMseqs2 server ([Mirdita et al. 2019, Bioinformatics](https://academic.oup.com/bioinformatics/article/35/16/2856/5280135)) for the multiple sequence alignment creation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfPhvYgKC81B"
      },
      "source": [
        "# License and Disclaimer\n",
        "\n",
        "This is not an officially-supported Google product.\n",
        "\n",
        "This Colab notebook and other information provided is for theoretical modelling only, caution should be exercised in its use. It is provided ‘as-is’ without any warranty of any kind, whether expressed or implied. Information is not intended to be a substitute for professional medical advice, diagnosis, or treatment, and does not constitute medical or other professional advice.\n",
        "\n",
        "Copyright 2021 DeepMind Technologies Limited.\n",
        "\n",
        "\n",
        "## AlphaFold Code License\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "## Model Parameters License\n",
        "\n",
        "The AlphaFold parameters are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "\n",
        "## Third-party software\n",
        "\n",
        "Use of the third-party software, libraries or code referred to in the [Acknowledgements section](https://github.com/deepmind/alphafold/#acknowledgements) in the AlphaFold README may be governed by separate terms and conditions or license provisions. Your use of the third-party software, libraries or code is subject to any such terms and you should check that you can comply with any applicable restrictions or terms and conditions before use.\n",
        "\n",
        "\n",
        "## Mirrored Databases\n",
        "\n",
        "The following databases have been mirrored by DeepMind, and are available with reference to the following:\n",
        "* UniProt: v2021\\_04 (unmodified), by The UniProt Consortium, available under a [Creative Commons Attribution-NoDerivatives 4.0 International License](http://creativecommons.org/licenses/by-nd/4.0/).\n",
        "* UniRef90: v2022\\_01 (unmodified), by The UniProt Consortium, available under a [Creative Commons Attribution-NoDerivatives 4.0 International License](http://creativecommons.org/licenses/by-nd/4.0/).\n",
        "* MGnify: v2022\\_05 (unmodified), by Mitchell AL et al., available free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use under [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n",
        "* BFD: (modified), by Steinegger M. and Söding J., modified by DeepMind, available under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by/4.0/). See the Methods section of the [AlphaFold proteome paper](https://www.nature.com/articles/s41586-021-03828-1) for details."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y aria2\n",
        "!pip install -U pip\n",
        "!pip install -r https://raw.githubusercontent.com/deepmind/alphafold/main/docker/requirements.txt\n",
        "\n",
        "# Step 2: Clone the AlphaFold repository\n",
        "!git clone https://github.com/deepmind/alphafold.git\n",
        "%cd alphafold\n",
        "\n",
        "# Step 3: Mount Google Drive to store genetic databases and output files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 4: Download necessary genetic databases\n",
        "# Specify the download directory in Google Drive\n",
        "download_dir = '/content/drive/MyDrive/alphafold_data'\n",
        "!bash scripts/download_all_data.sh $download_dir\n",
        "\n",
        "# Note: This will take a long time (~556 GB to download). You can run this in the background.\n",
        "# You may want to consider downloading the databases on your local machine if this step takes too long in Colab.\n",
        "\n",
        "# Step 5: Upload the protein sequence file in FASTA format\n",
        "# Use Colab's file upload feature to upload your FASTA file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# After uploading the file, you can access it from `/content` (Colab's working directory)\n",
        "\n",
        "# Step 6: Run AlphaFold prediction\n",
        "# Replace 'your_protein.fasta' with the name of your uploaded FASTA file\n",
        "fasta_file = '/content/your_protein.fasta'  # Change this to your actual file name\n",
        "output_dir = '/content/alphafold_output'\n",
        "\n",
        "!python3 docker/run_docker.py \\\n",
        "  --fasta_paths=$fasta_file \\\n",
        "  --max_template_date=2022-01-01 \\\n",
        "  --data_dir=$download_dir \\\n",
        "  --output_dir=$output_dir\n",
        "\n",
        "# Step 7: Visualizing the output (optional)\n",
        "# Install PyMOL to visualize the predicted structure (if desired)\n",
        "!pip install pymol\n",
        "\n",
        "# Visualize the predicted structure (update the path to match the output file)\n",
        "from pymol import cmd\n",
        "cmd.load(f\"{output_dir}/your_predicted_structure.pdb\")  # Replace with the actual output file path\n",
        "cmd.show(\"cartoon\")\n",
        "cmd.color(\"blue\", \"chain A\")\n",
        "\n",
        "# Optional: To download the predicted structure file, you can use:\n",
        "# files.download(f'{output_dir}/your_predicted_structure.pdb')  # Replace with actual output file path\n"
      ],
      "metadata": {
        "id": "T-rPsiso_UQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}