{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-w93m3wgcWp"
      },
      "source": [
        "```\n",
        "Copyright 2022 DeepMind Technologies Limited\n",
        "\n",
        "All software is licensed under the Apache License, Version 2.0 (Apache 2.0);\n",
        "you may not use this file except in compliance with the Apache 2.0 license.\n",
        "You may obtain a copy of the Apache 2.0 license at:\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "All other materials are licensed under the Creative Commons Attribution 4.0\n",
        "International License (CC-BY). You may obtain a copy of the CC-BY license at:\n",
        "https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, all software and\n",
        "materials distributed here under the Apache 2.0 or CC-BY licenses are\n",
        "distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
        "either express or implied. See the licenses for the specific language governing\n",
        "permissions and limitations under those licenses.\n",
        "\n",
        "This is not an official Google product.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bZIBq3JJLT"
      },
      "source": [
        "# JAX Implementation of LINAC Defence for CIFAR-10 Images\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook contains code for reproducing the LINAC transform introduced\n",
        "in the ICML 2022 paper: [\"Hindering Adversarial Attacks with Implicit Neural\n",
        "Representations\"](https://proceedings.mlr.press/v162/rusu22a.html) by Andrei A Rusu, Dan Andrei Calian, Sven Gowal and Raia Hadsel.\n",
        "\n",
        "### Abstract\n",
        "\n",
        "We introduce the **Lossy Implicit Network Activation Coding (LINAC)** defence, an input transformation which successfully hinders several common adversarial attacks on CIFAR-10 classifiers for perturbations up to 8/255 in Linf norm and 0.5 in L2 norm. **Implicit neural representations (INRs)** are used to approximately encode pixel colour intensities in 2D images such that classifiers trained on transformed data appear to have robustness to small perturbations without adversarial training or large drops in performance. The seed of the random number generator used to initialise and train INRs turns out to be necessary information for stronger generic attacks, suggesting its role as a private key. We devise a Parametric Bypass Approximation (PBA) attack strategy for key-based defences, which successfully invalidates an existing method in this category. Interestingly, our LINAC defence also hinders some transfer and adaptive attacks, including our novel PBA strategy. Our results emphasise the importance of a broad range of customised attacks despite apparent robustness according to standard evaluations.\n",
        "\n",
        "## Implementation details\n",
        "\n",
        "The Haiku module and defence function provided below can be used to instantiate new classifiers, and to load and perform inferences with the model we evaluated in the paper.\n",
        "\n",
        "The code is CPU, GPU and TPU compatible.\n",
        "\n",
        "Note that LINAC was only evaluated with the pre-processing pipeline below.\n",
        "\n",
        "## Notebook overview:\n",
        "* We provide example code for computing the LINAC transform and appropriately configuring the defence.\n",
        "* We share the parameters of the LINAC defended classifier evaluated in our paper.\n",
        "* We give example JAX code for loading and performing inference with this model.\n",
        "  * *We invite future works to evaluate its apparent robustness!*\n",
        "* We plot representations and reconstructions of a couple of CIFAR-10 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLufCiP_PxHC"
      },
      "outputs": [],
      "source": [
        "#@title Downloads and Setup\n",
        "\n",
        "!pip install dm-haiku\n",
        "!pip install optax\n",
        "!git clone https://github.com/deepmind/deepmind-research\n",
        "\n",
        "\n",
        "# Download the LINAC defended model evaluated throughout our paper.\n",
        "!wget https://storage.googleapis.com/dm-adversarial-robustness/linac/cifar10_linac_wrn70-16.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jRC8b-l74MDA"
      },
      "outputs": [],
      "source": [
        "#@title Importing Packages\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "# Make TF unaware of the GPU.\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "tf.config.set_visible_devices([], 'TPU')\n",
        "\n",
        "from absl import logging\n",
        "from optax import global_norm\n",
        "\n",
        "\n",
        "import functools\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import sys\n",
        "\n",
        "sys.path.append('deepmind-research')\n",
        "from adversarial_robustness.jax import model_zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vDzxoGlDQFXJ"
      },
      "outputs": [],
      "source": [
        "#@title Utility Code\n",
        "\n",
        "\n",
        "def plot_image(ax, image):\n",
        "  \"\"\"Normalizes and plots image.\"\"\"\n",
        "  image_min = image.min()\n",
        "  image_max = image.max() + 1e-10\n",
        "  if image.ndim == 3 and image.shape[-1] in [3, 4]:\n",
        "    image = np.copy(image)\n",
        "    image -= image_min\n",
        "    image /= (image_max - image_min)\n",
        "\n",
        "  ax.imshow(image, vmin=image_min, vmax=image_max,\n",
        "            interpolation='none')\n",
        "\n",
        "  ax.grid(False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  return ax\n",
        "\n",
        "\n",
        "def compare_inputs_to_reconstructions(images,\n",
        "                                      reconstructions,\n",
        "                                      n_channels=3,\n",
        "                                      plot_sz=3):\n",
        "  \"\"\"Displays images, reconstructions via INRs and deltas side-by-side.\"\"\"\n",
        "  n_images = min(images.shape[0], 10)\n",
        "  image = images[0]\n",
        "  n_concat_image = image.shape[-1]//n_channels\n",
        "\n",
        "  fig, axarr = plt.subplots(n_concat_image * n_images, 3)\n",
        "  fig.set_size_inches((axarr.shape[-1] * plot_sz,\n",
        "                       axarr.shape[0] * plot_sz))\n",
        "\n",
        "  for k in range(n_images):\n",
        "    image = images[k]\n",
        "    n_concat_image = image.shape[-1]//n_channels\n",
        "    for j in range(n_concat_image):\n",
        "      concat_image = image[:, :, (j*n_channels):(j+1)*n_channels]\n",
        "\n",
        "      # Show image.\n",
        "      ax = axarr[k + j][-3]\n",
        "      ax = plot_image(ax, concat_image)\n",
        "\n",
        "      # Show reconstruction image.\n",
        "      ax = axarr[k + j][-2]\n",
        "      ax = plot_image(ax, reconstructions[k])\n",
        "\n",
        "      # Show delta as image.\n",
        "      ax = axarr[k + j][-1]\n",
        "      diff_image = reconstructions[k] - images[k][:, :, 0:3]\n",
        "      loss_batch = jnp.mean(jnp.sum(jnp.square(diff_image), axis=-1))\n",
        "      ax = plot_image(ax, diff_image)\n",
        "      ax.set_title('{:1.2e}'.format(loss_batch))\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_linac_reps_on_color_channels(input_images,\n",
        "                                      image_reps,\n",
        "                                      image_indices=(0, 1, 2),\n",
        "                                      num_layers=1,\n",
        "                                      num_units=256,\n",
        "                                      plot_sz=1,\n",
        "                                      rows_per_layer=16,\n",
        "                                      max_units_per_layer=None):\n",
        "  \"\"\"Plots LINAC representations for up to three indices in batch.\"\"\"\n",
        "  if not isinstance(image_indices, (list, tuple)):\n",
        "    image_indices = [image_indices] * 3\n",
        "\n",
        "  if max_units_per_layer is not None:\n",
        "    if num_units > max_units_per_layer:\n",
        "      num_units = max_units_per_layer\n",
        "\n",
        "  nrows = num_layers * rows_per_layer\n",
        "  ncols = num_units // rows_per_layer // num_layers\n",
        "\n",
        "  image_rep = np.array(image_reps[image_indices, :, :, :num_units])\n",
        "\n",
        "  image_rep_max = image_rep.max(axis=1, keepdims=True)\n",
        "  image_rep_max = image_rep_max.max(axis=2, keepdims=True)\n",
        "  image_rep /= np.maximum(1., image_rep_max + 1e-10)\n",
        "\n",
        "  image_rep = image_rep.reshape(image_rep.shape[:3] + (nrows, ncols))\n",
        "\n",
        "  # Add white borders to all sides of images.\n",
        "  border = np.ones(shape=image_rep.max(axis=1, keepdims=True).shape)\n",
        "  image_rep = np.concatenate([border, image_rep, border], axis=1)\n",
        "  border = np.ones(shape=image_rep.max(axis=2, keepdims=True).shape)\n",
        "  image_rep = np.concatenate([border, image_rep, border], axis=2)\n",
        "\n",
        "  # Plot selected images.\n",
        "  fig, axarr = plt.subplots(1, len(image_indices))\n",
        "  fig.set_size_inches((plot_sz*2 * len(axarr), plot_sz*2))\n",
        "  colour = ['Red', 'Green', 'Blue']\n",
        "  for k, ind in enumerate(image_indices):\n",
        "    plot_image(axarr[k], input_images[ind])\n",
        "    axarr[k].grid(False)\n",
        "    axarr[k].set_title(colour[k])\n",
        "  plt.show()\n",
        "\n",
        "  image_rep = image_rep.transpose([3, 1, 4, 2, 0])\n",
        "\n",
        "  image_rep = image_rep.reshape((image_rep.shape[0]*image_rep.shape[1],\n",
        "                                 image_rep.shape[2]*image_rep.shape[3],\n",
        "                                 image_rep.shape[4]))\n",
        "\n",
        "  fig = plt.figure(figsize=(ncols*plot_sz, nrows*plot_sz))\n",
        "  ax = fig.gca()\n",
        "  ax.set_facecolor('black')\n",
        "  ax.imshow(image_rep, cmap='gray', interpolation='nearest')\n",
        "\n",
        "  ax.set_xlabel('hidden unit position in layer')\n",
        "  ax.set_xticks(range(image_rep.shape[1]//ncols//2,\n",
        "                      image_rep.shape[1],\n",
        "                      image_rep.shape[1]//ncols))\n",
        "  ax.set_xticklabels(range(1, ncols + 1))\n",
        "\n",
        "  ax.set_ylabel('hidden layer')\n",
        "  ax.set_yticks(range(0, image_rep.shape[0],\n",
        "                      image_rep.shape[0]//num_layers))\n",
        "  ax.set_yticklabels(range(1, num_layers + 1))\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "shhD7WK367kP"
      },
      "outputs": [],
      "source": [
        "#@title Data Loading\n",
        "\n",
        "MEANS = (0.4914, 0.4822, 0.4465)\n",
        "STDEVS = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "\n",
        "def normalize_image(image, label):\n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "  return (image - MEANS) / STDEVS, label\n",
        "\n",
        "\n",
        "ds = tfds.load('cifar10', split='test', as_supervised=True,\n",
        "               with_info=False)\n",
        "ds = ds.map(normalize_image).cache().batch(8)\n",
        "\n",
        "image_batch, label_batch = next(iter(tfds.as_numpy(ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIrGIvh4f8wE"
      },
      "source": [
        "# LINAC Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ajn7p1brgV0v"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "\n",
        "\n",
        "def log_params(name, params, concrete=False, verbose=True):\n",
        "  \"\"\"Prints stats of parameter set.\"\"\"\n",
        "  def predicate_not_counter(module_name, name, value):\n",
        "    del module_name, value\n",
        "    substr_to_filter = [\n",
        "        'counter',\n",
        "    ]\n",
        "    return not np.any([substr in name for substr in substr_to_filter])\n",
        "\n",
        "  # Filter out counters in states.\n",
        "  params, excluded_params = hk.data_structures.partition(\n",
        "      predicate_not_counter, params)\n",
        "  num_excluded_params = hk.data_structures.tree_size(excluded_params)\n",
        "  if num_excluded_params > 0:\n",
        "    logging.info('log_params: WARNING %s excluded params: %d', name,\n",
        "                 num_excluded_params)\n",
        "\n",
        "  num_params = hk.data_structures.tree_size(params)\n",
        "  byte_size = hk.data_structures.tree_bytes(params)\n",
        "  logging.info('log_params: %s count: %d, size: %.2fMB', name, num_params,\n",
        "               byte_size / 1e6)\n",
        "  if concrete:\n",
        "    param_shapes = jax.tree_map(\n",
        "        lambda x: (x.shape, jnp.squeeze(jnp.sqrt(jnp.sum(jnp.square(x))))),\n",
        "        params)\n",
        "  else:\n",
        "    param_shapes = jax.tree_map(lambda x: (x.shape,), params)\n",
        "\n",
        "  if verbose:\n",
        "    logging.info('log_params: %s shapes/L2:\\n%s', name, str(param_shapes))\n",
        "  if concrete:\n",
        "    sum_of_squares = 0\n",
        "    for _, _, values in hk.data_structures.traverse(params):\n",
        "      sum_of_squares += jnp.sum(jnp.square(values))\n",
        "    logging.info('log_params: %s L2 norm: %.2f\\n', name,\n",
        "                 jnp.sqrt(sum_of_squares))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aeg2uxtk9vB9"
      },
      "outputs": [],
      "source": [
        "#@title Training INRs\n",
        "\n",
        "def _transform_input(x: jnp.ndarray,\n",
        "                     num_features: int):\n",
        "  \"\"\"Projects input into higher dimensions.\"\"\"\n",
        "  output = jnp.concatenate(\n",
        "      [jnp.sin((2.**i) * jnp.pi * x) for i in range(num_features)] +\n",
        "      [jnp.cos((2.**i) * jnp.pi * x) for i in range(num_features)],\n",
        "      axis=-1)\n",
        "  total_num_features = x.shape[1] * 2 * num_features\n",
        "  errmsg = f'{output.shape[1]} != {total_num_features}'\n",
        "  assert output.shape[1] == total_num_features, errmsg\n",
        "\n",
        "  assert len(output.shape) == 2\n",
        "  assert output.shape[0] == x.shape[0]\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def _inr_add_layer(ind, sz, output, with_bias, activation,\n",
        "                   layer_name_suffix='linear'):\n",
        "  \"\"\"Adds a layer to the INR model.\"\"\"\n",
        "  output = hk.Linear(\n",
        "      output_size=sz,\n",
        "      with_bias=with_bias,\n",
        "      name=f'layer_{ind:03d}_{layer_name_suffix}')(\n",
        "          output)\n",
        "\n",
        "  if activation is not None:\n",
        "    return activation(output)\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def inr_forward_fn(x: jnp.ndarray,\n",
        "                   output_dims,\n",
        "                   num_layers,\n",
        "                   layer_sz,\n",
        "                   num_features,\n",
        "                   with_bias,\n",
        "                   activation,\n",
        "                   representation_layer_index,\n",
        "                   output_representation=False):\n",
        "  \"\"\"Define forward computations of implicit networks.\"\"\"\n",
        "\n",
        "  output_sizes = [layer_sz]*num_layers + [output_dims]\n",
        "  output = _transform_input(x, num_features=num_features)\n",
        "\n",
        "  if output_representation:\n",
        "    errmsg = (f'representation_layer_index cannot be '\n",
        "              f'negative ({representation_layer_index})')\n",
        "    assert representation_layer_index >= 0, errmsg\n",
        "    errmsg = (f'representation_layer_index > len(output_sizes) since '\n",
        "              f'{representation_layer_index} > {len(output_sizes)}')\n",
        "    assert representation_layer_index <= len(output_sizes), errmsg\n",
        "    representation = []\n",
        "\n",
        "  for l, sz in enumerate(output_sizes[:-1]):\n",
        "    output = _inr_add_layer(l, sz, output, with_bias=with_bias,\n",
        "                            activation=activation,\n",
        "                            layer_name_suffix='adaptive')\n",
        "\n",
        "    if output_representation:\n",
        "      representation.append(output)\n",
        "\n",
        "  # Add the last layer.\n",
        "  l = len(output_sizes) - 1\n",
        "  # Final layer.\n",
        "  sz = output_sizes[-1]\n",
        "  output = _inr_add_layer(l, sz, output, with_bias=True, activation=None,\n",
        "                          layer_name_suffix='adaptive_output')\n",
        "  # Done constructing the implicit network.\n",
        "  assert output.shape[-1] == output_dims\n",
        "\n",
        "  # Add output layer activations to representation.\n",
        "  if output_representation:\n",
        "    representation.append(output)\n",
        "    assert len(representation) == len(output_sizes), 'missed some reps'\n",
        "\n",
        "  if output_representation:\n",
        "    errmsg = 'representation_layer_index too large'\n",
        "    assert len(representation) >= representation_layer_index, errmsg\n",
        "    reps = representation[representation_layer_index]\n",
        "    logging.info('reps.shape: %s', reps.shape)\n",
        "    return reps\n",
        "  else:\n",
        "    return output\n",
        "\n",
        "\n",
        "def inr_get_epoch_batches(key, data, batch_size, verbose_logs):\n",
        "  \"\"\"Shuffles the data for one epoch and groups into batches.\"\"\"\n",
        "  inputs, targets = data\n",
        "  del data\n",
        "  if verbose_logs:\n",
        "    logging.info('inr_get_epoch_batches: inputs.shape: %s', inputs.shape)\n",
        "    logging.info('inr_get_epoch_batches: targets.shape: %s', targets.shape)\n",
        "\n",
        "  # Total number of pixels.\n",
        "  num_inputs = inputs.shape[0]\n",
        "  # Total number of pixel batches.\n",
        "  num_batches = num_inputs // batch_size\n",
        "\n",
        "  # Split pixels randomly into batches for this epoch.\n",
        "  key, subkey = jax.random.split(key)\n",
        "  epoch_perm = jax.random.permutation(subkey, jnp.arange(num_inputs))\n",
        "  del subkey\n",
        "  shuffled_inputs = inputs[epoch_perm]\n",
        "  shuffled_targets = targets[epoch_perm]\n",
        "  errmsg = f'{shuffled_inputs.shape[0]} != {inputs.shape[0]}'\n",
        "  assert shuffled_inputs.shape[0] == inputs.shape[0], errmsg\n",
        "  errmsg = f'{shuffled_targets.shape[0]} != {targets.shape[0]}'\n",
        "  assert shuffled_targets.shape[0] == targets.shape[0], errmsg\n",
        "  errmsg = f'{shuffled_targets.shape[0]} != {shuffled_inputs.shape[0]}'\n",
        "  assert shuffled_targets.shape[0] == shuffled_inputs.shape[0], errmsg\n",
        "\n",
        "  epoch_batches = (\n",
        "      shuffled_inputs.reshape((num_batches, batch_size) +\n",
        "                              shuffled_inputs.shape[1:]),\n",
        "      shuffled_targets.reshape((num_batches, batch_size) +\n",
        "                               shuffled_targets.shape[1:]))\n",
        "  assert epoch_batches[0].shape[0] == num_batches\n",
        "  assert epoch_batches[1].shape[0] == num_batches\n",
        "  if verbose_logs:\n",
        "    logging.info('inr_get_epoch_batches: epoch_batches[0].shape: %s',\n",
        "                 epoch_batches[0].shape)\n",
        "    logging.info('inr_get_epoch_batches: epoch_batches[1].shape: %s',\n",
        "                 epoch_batches[1].shape)\n",
        "  return epoch_batches, epoch_perm\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames=['apply_fn'])\n",
        "def loss_fn(params, state, inputs, targets, apply_fn):\n",
        "  \"\"\"INR loss function.\"\"\"\n",
        "\n",
        "  assert inputs.ndim == 2, f'loss_fn: inputs.ndim: {inputs.ndim} != 2'\n",
        "  assert targets.ndim == 2, f'loss_fn: targets.ndim: {targets.ndim} != 2'\n",
        "\n",
        "  preds, state = apply_fn(params, state, inputs)\n",
        "\n",
        "  assert preds.ndim == 2, f'loss_fn: preds.ndim: {preds.ndim} != 2'\n",
        "  errmsg = f'preds.shape != targets.shape: {preds.shape} != {targets.shape}'\n",
        "  assert np.all(np.equal(preds.shape, targets.shape)), errmsg\n",
        "\n",
        "  batch_loss = jnp.mean(jnp.sum(jnp.square(preds - targets), axis=1))\n",
        "  return batch_loss, state\n",
        "\n",
        "\n",
        "loss_value_and_grad_fn = jax.jit(\n",
        "    jax.value_and_grad(loss_fn, argnums=0, has_aux=True),\n",
        "    static_argnames=['apply_fn'])\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.jit, static_argnames=['apply_fn', 'opt_update', 'verbose_logs'])\n",
        "def _inr_train_one_step(carry, step_data, apply_fn, opt_update, verbose_logs):\n",
        "  \"\"\"Does one step of INR training.\"\"\"\n",
        "  params, state, optim_state = carry\n",
        "  train_inputs, train_targets = step_data\n",
        "  del verbose_logs\n",
        "\n",
        "  (train_step_loss, train_state), train_grad = loss_value_and_grad_fn(\n",
        "      params, state=state,\n",
        "      inputs=train_inputs, targets=train_targets, apply_fn=apply_fn)\n",
        "\n",
        "  train_updates, train_optim_state = opt_update(\n",
        "      train_grad, state=optim_state, params=params)\n",
        "\n",
        "  # Update carry.\n",
        "  params = optax.apply_updates(\n",
        "      params=params, updates=train_updates)\n",
        "  state = train_state\n",
        "  optim_state = train_optim_state\n",
        "  carry = (params, state, optim_state)\n",
        "\n",
        "  # Gather stats.\n",
        "  l2_params = global_norm(params)\n",
        "  l2_optim_state = global_norm(optim_state)\n",
        "  l2_step_data = global_norm(step_data)\n",
        "  l2_train_grad = global_norm(train_grad)\n",
        "  l2_train_updates = global_norm(train_updates)\n",
        "  step_train_stats = (l2_params, l2_optim_state, l2_step_data,\n",
        "                      l2_train_grad, l2_train_updates)\n",
        "  aux = (train_step_loss, step_train_stats)\n",
        "\n",
        "  return carry, aux\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.jit, static_argnames=['apply_fn', 'opt_update', 'batch_size',\n",
        "                              'verbose_logs'])\n",
        "def _inr_train_one_epoch(carry, key, train_data, apply_fn, opt_update,\n",
        "                         batch_size, verbose_logs):\n",
        "  \"\"\"Trains the INR of a single image for one epoch.\"\"\"\n",
        "\n",
        "  inr_train_one_step = functools.partial(\n",
        "      _inr_train_one_step,\n",
        "      apply_fn=apply_fn, opt_update=opt_update, verbose_logs=verbose_logs)\n",
        "\n",
        "  key, subkey = jax.random.split(key)\n",
        "  epoch_data, epoch_perm = inr_get_epoch_batches(\n",
        "      key=subkey, data=train_data, batch_size=batch_size,\n",
        "      verbose_logs=verbose_logs)\n",
        "  del subkey\n",
        "\n",
        "  out_carry, out_aux = jax.lax.scan(\n",
        "      inr_train_one_step, carry, epoch_data)\n",
        "\n",
        "  train_step_loss, epoch_train_stats = out_aux\n",
        "\n",
        "  if verbose_logs:\n",
        "    logging.info('_inr_train_one_epoch: train_step_loss.shape: %s',\n",
        "                 train_step_loss.shape)\n",
        "\n",
        "  return out_carry, (train_step_loss, epoch_train_stats, epoch_perm)\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.jit, static_argnames=['apply_fn', 'opt_update', 'batch_size',\n",
        "                              'num_epochs', 'verbose_logs'])\n",
        "def inr_train(carry, key, train_data, apply_fn,\n",
        "              opt_update, batch_size, num_epochs, verbose_logs):\n",
        "  \"\"\"Trains the INR of a single input image for several epochs.\"\"\"\n",
        "\n",
        "  inr_train_one_epoch = functools.partial(\n",
        "      _inr_train_one_epoch,\n",
        "      train_data=train_data,\n",
        "      apply_fn=apply_fn,\n",
        "      opt_update=opt_update,\n",
        "      batch_size=batch_size,\n",
        "      verbose_logs=verbose_logs)\n",
        "\n",
        "  # Generate distinct rng keys for each epoch in order to permute pixels\n",
        "  # differently in each one.\n",
        "  subkeys = jnp.asarray(jax.random.split(key, num_epochs))\n",
        "  del key\n",
        "  assert subkeys.ndim == 2, f'subkeys.ndim: {subkeys.ndim} != 2'\n",
        "  errmsg = f'subkeys.shape: {subkeys.shape} leading dim not {num_epochs}'\n",
        "  assert subkeys.shape[0] == num_epochs, errmsg\n",
        "  errmsg = f'subkeys.shape: {subkeys.shape} third dim not 2'\n",
        "  assert subkeys.shape[1] == 2, errmsg\n",
        "\n",
        "  out_carry, out_aux = jax.lax.scan(inr_train_one_epoch, carry, subkeys)\n",
        "\n",
        "  if verbose_logs:\n",
        "    train_losses, _, _ = out_aux\n",
        "    logging.info('inr_train: train_losses.shape: %s', train_losses.shape)\n",
        "\n",
        "  return out_carry, out_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AmaaC69Og2EE"
      },
      "outputs": [],
      "source": [
        "#@title LINAC Haiku Module\n",
        "\n",
        "\n",
        "class LINAC(hk.Module):\n",
        "  \"\"\"Transforms RGB images by memorizing with a fixed initial neural network.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               config,\n",
        "               verbose_logs: bool,\n",
        "               name=None):\n",
        "    \"\"\"Creates a LINAC module.\n",
        "\n",
        "    Args:\n",
        "      config: configuration dictionary for INR computations.\n",
        "      verbose_logs: whether to output log verbosely.\n",
        "      name: name of module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self._config = config\n",
        "    self._verbose_logs = verbose_logs\n",
        "\n",
        "  def __call__(self,\n",
        "               images: jnp.ndarray,\n",
        "               private_key: np.int64):\n",
        "    \"\"\"Apply LINAC transform.\"\"\"\n",
        "\n",
        "    rng = jax.random.PRNGKey(private_key)\n",
        "    rng, model_init_key = jax.random.split(rng)\n",
        "    rng, _ = jax.random.split(rng)\n",
        "    rng, training_key = jax.random.split(rng)\n",
        "    rng, shuffle_key = jax.random.split(rng)\n",
        "    del rng, private_key\n",
        "\n",
        "    errmsg = 'transform works with 2D images with at least one channel'\n",
        "    assert len(images.shape) >= 3, errmsg\n",
        "    image_height, image_width, num_channels = images.shape[-3:]\n",
        "    num_spatial_dim = 2\n",
        "    # Flatted spatial dimensions of input images.\n",
        "    images = hk.Reshape(\n",
        "        output_shape=(image_height*image_width, num_channels))(images)\n",
        "\n",
        "    # Flattened input grid.\n",
        "    grid_offset = 1\n",
        "    w = jnp.linspace(-1., 1., num=image_width + 2 * grid_offset)\n",
        "    w = w[grid_offset:-grid_offset if grid_offset > 0 else None]\n",
        "    h = jnp.linspace(1., -1., num=image_height + 2 * grid_offset)\n",
        "    h = h[grid_offset:-grid_offset if grid_offset > 0 else None]\n",
        "    x, y = jnp.meshgrid(w, h)\n",
        "    xy = jnp.stack([x.reshape([-1]), y.reshape([-1])], axis=-1)\n",
        "    input_grid = xy.astype(jnp.float32)\n",
        "    logging.info('input_grid.shape: %s', input_grid.shape)\n",
        "\n",
        "    # Flattened represenation grid.\n",
        "    rep_grid = input_grid\n",
        "    logging.info('rep_grid.shape: %s', rep_grid.shape)\n",
        "\n",
        "    # Configure INR model.\n",
        "    output_dims = num_channels\n",
        "    config = self._config\n",
        "    if self._verbose_logs:\n",
        "      for k, v in config.items():\n",
        "        logging.info('%s: %s', k, v)\n",
        "\n",
        "    inr_model = hk.without_apply_rng(hk.transform_with_state(functools.partial(\n",
        "        inr_forward_fn, output_dims=output_dims,\n",
        "        representation_layer_index=config['representation_layer_index'],\n",
        "        **config['inr'], output_representation=False)))\n",
        "\n",
        "    # Generate the shared initialisation for all implicit networks.\n",
        "    init_params, init_states = inr_model.init(\n",
        "        model_init_key, jnp.zeros([1, num_spatial_dim]))\n",
        "\n",
        "    # Total number of INR fitting steps.\n",
        "    num_steps = (image_height * image_width) // config['batch_size']\n",
        "    num_steps *= config['num_epochs']\n",
        "    if self._verbose_logs:\n",
        "      logging.info('num_steps: %d', num_steps)\n",
        "\n",
        "    # Use a decay schedule with Adam.\n",
        "    alpha = config['cosine_decay_schedule_alpha']\n",
        "    lr_fn = optax.cosine_decay_schedule(\n",
        "        config['adam']['learning_rate'], num_steps, alpha)\n",
        "    config['adam']['learning_rate'] = lr_fn\n",
        "\n",
        "    # Shuffle input grid.\n",
        "    assert input_grid.ndim == num_spatial_dim\n",
        "    num_inputs = input_grid.shape[0]\n",
        "    assert input_grid.shape[1] == num_spatial_dim\n",
        "    grid_shuffle_perm = jax.random.permutation(shuffle_key,\n",
        "                                               jnp.arange(num_inputs))\n",
        "    shuffled_input_grid = input_grid[grid_shuffle_perm]\n",
        "    assert shuffled_input_grid.shape[0] == input_grid.shape[\n",
        "        0], f'{shuffled_input_grid.shape[0]} != {input_grid.shape[0]}'\n",
        "    train_grid = shuffled_input_grid\n",
        "\n",
        "    if self._verbose_logs:\n",
        "      logging.info('train_grid.shape: %s', train_grid.shape)\n",
        "\n",
        "    opt_init, opt_update = optax.adam(**config['adam'])\n",
        "\n",
        "    if self._verbose_logs:\n",
        "      log_params('init_params', init_params, concrete=False, verbose=True)\n",
        "\n",
        "    # Initialise the model's parameters and the optimiser's states.\n",
        "    init_opt_states = opt_init(init_params)\n",
        "\n",
        "    if self._verbose_logs:\n",
        "      log_params(\n",
        "          'init_states',\n",
        "          init_states,\n",
        "          concrete=False,\n",
        "          verbose=self._verbose_logs)\n",
        "      logging.info('init_opt_states: %s',\n",
        "                   jax.tree_map(lambda x: (x.shape), init_opt_states))\n",
        "\n",
        "    train_pixels = images[:, grid_shuffle_perm, :]\n",
        "    if self._verbose_logs:\n",
        "      logging.info('train_pixels: %s', train_pixels.shape)\n",
        "    del images\n",
        "\n",
        "    carry = (init_params, init_states, init_opt_states)\n",
        "\n",
        "    # Train independent INRs for each input image in parallel.\n",
        "    batch_inr_train = jax.jit(\n",
        "        jax.vmap(\n",
        "            functools.partial(\n",
        "                inr_train,\n",
        "                apply_fn=inr_model.apply,\n",
        "                opt_update=opt_update,\n",
        "                batch_size=config['batch_size'],\n",
        "                num_epochs=config['num_epochs'],\n",
        "                verbose_logs=self._verbose_logs),\n",
        "            in_axes=[None, None, (None, 0)]))\n",
        "\n",
        "    # Return INRs for all images.\n",
        "    out_carry, out_aux = batch_inr_train(\n",
        "        carry, training_key, (train_grid, train_pixels))\n",
        "\n",
        "    inr_params, inr_states, _ = out_carry\n",
        "    if self._verbose_logs:\n",
        "      log_params('inr_params', inr_params, concrete=False, verbose=True)\n",
        "      log_params('inr_states', inr_states, concrete=False, verbose=True)\n",
        "\n",
        "    # Use INRs to compute image specific representations.\n",
        "    activation_coding = hk.without_apply_rng(hk.transform_with_state(\n",
        "        functools.partial(\n",
        "            inr_forward_fn, output_dims=output_dims,\n",
        "            representation_layer_index=config['representation_layer_index'],\n",
        "            **config['inr'], output_representation=True)))\n",
        "\n",
        "    @jax.jit\n",
        "    def batch_rep_fn(params, states):\n",
        "      vfn = jax.vmap(activation_coding.apply, in_axes=[0, 0, None])\n",
        "      return vfn(params, states, rep_grid)\n",
        "\n",
        "    # Undo flattening of spatial dimensions and output 2D images.\n",
        "    linac_reps, _ = batch_rep_fn(inr_params, inr_states)\n",
        "    output_shape = (image_height, image_width, linac_reps.shape[-1])\n",
        "    linac_reps = hk.Reshape(output_shape=output_shape)(linac_reps)\n",
        "\n",
        "    return linac_reps, out_carry, out_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K2c5C4axg15u"
      },
      "outputs": [],
      "source": [
        "#@title LINAC Defence\n",
        "\n",
        "\n",
        "def linac_defence(inputs,\n",
        "                  private_key=-2314326399425823309,\n",
        "                  inr_num_layers=5,\n",
        "                  inr_layer_sz=256,\n",
        "                  inr_num_features=5,\n",
        "                  inr_with_bias=True,\n",
        "                  inr_activation=jax.nn.relu,\n",
        "                  representation_layer_index=2,\n",
        "                  num_epochs=10,\n",
        "                  batch_size=32,\n",
        "                  adam_learning_rate=1e-3,\n",
        "                  adam_b1=0.9,\n",
        "                  adam_b2=0.99,\n",
        "                  adam_eps=1e-8,\n",
        "                  cosine_decay_schedule_alpha=1e-4,\n",
        "                  verbose_logs=False):\n",
        "  \"\"\"Transforms inputs using LINAC with paper defaults.\"\"\"\n",
        "\n",
        "  linac_config = dict(\n",
        "      inr=dict(\n",
        "          num_layers=inr_num_layers,\n",
        "          layer_sz=inr_layer_sz,\n",
        "          num_features=inr_num_features,\n",
        "          with_bias=inr_with_bias,\n",
        "          activation=inr_activation),\n",
        "      representation_layer_index=representation_layer_index,\n",
        "      num_epochs=num_epochs,\n",
        "      batch_size=batch_size,\n",
        "      cosine_decay_schedule_alpha=cosine_decay_schedule_alpha,\n",
        "      adam=dict(\n",
        "          learning_rate=adam_learning_rate,\n",
        "          b1=adam_b1,\n",
        "          b2=adam_b2,\n",
        "          eps=adam_eps))\n",
        "\n",
        "  transformation = LINAC(config=linac_config, verbose_logs=verbose_logs)\n",
        "\n",
        "  out = transformation(inputs, private_key=private_key)\n",
        "\n",
        "  out_reps, out_params_and_states, out_train_stats = out\n",
        "  return out_reps, out_params_and_states, out_train_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fHFhqckjNS"
      },
      "source": [
        "# Using the defended classifier evaluated in the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbXxac5qkrrQ"
      },
      "outputs": [],
      "source": [
        "# Instantiates a LINAC defended classifier.\n",
        "@hk.transform_with_state\n",
        "def defended_model_fn(x: jnp.ndarray, is_training=False):\n",
        "  \"\"\"Sets up LINAC defended classifier.\"\"\"\n",
        "  model = model_zoo.WideResNet(\n",
        "      num_classes=10,\n",
        "      depth=70,\n",
        "      width=16,\n",
        "      activation='swish',\n",
        "      norm_args={\n",
        "          'create_offset': False,\n",
        "          'create_scale': True,\n",
        "          'decay_rate': .99,\n",
        "      })\n",
        "  return model(linac_defence(x)[0], is_training=is_training)\n",
        "\n",
        "\n",
        "# Defines inference function using the loaded classifier.\n",
        "params, state = np.load('cifar10_linac_wrn70-16.npy', allow_pickle=True)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "\n",
        "def logits_fn(x):\n",
        "  return defended_model_fn.apply(params, state, rng, x)[0]\n",
        "\n",
        "\n",
        "# Evaluate defended classifier on image batch.\n",
        "logits_batch = logits_fn(image_batch)\n",
        "\n",
        "\n",
        "# Print batch accuracy.\n",
        "predictions_batch = jnp.argmax(logits_batch, axis=-1)\n",
        "correct = (predictions_batch == label_batch).sum()\n",
        "num_images = image_batch.shape[0]\n",
        "batch_accuracy = correct * 100./num_images\n",
        "print('num_images: {:d} batch_accuracy: {:2.2f}%'.format(\n",
        "    num_images, batch_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiFzvmZlxFzv"
      },
      "source": [
        "### Evaluate LINAC defended classifier on the full CIFAR-10 test set\n",
        "\n",
        "CIFAR-10 test set accuracy reported in the paper: 93.08%.\n",
        "\n",
        "Code is commented out to speed up notebook execution. Please note that calling `logits_fn` without `jax.jit` and a modern GPU/TPU can result in slow evaluation on the full CIFAR-10 test set, taking over 1 hour. In contrast, using `jax.jit` will make this evaluation finish in about 5 minutes, but may lead to out of memory errors on some GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO2DKZnhxLwg"
      },
      "outputs": [],
      "source": [
        "# correct = 0\n",
        "# num_images = 0\n",
        "\n",
        "# for image_batch, label_batch in tfds.as_numpy(ds):\n",
        "#   logits_batch = logits_fn(image_batch)\n",
        "\n",
        "#   # Update statistics.\n",
        "#   predictions_batch = jnp.argmax(logits_batch, axis=-1)\n",
        "#   correct += (predictions_batch == label_batch).sum()\n",
        "#   num_images += image_batch.shape[0]\n",
        "\n",
        "# accuracy = correct * 100. / num_images\n",
        "# print('num_images: {:d} test set accuracy: {:2.2f}%'.format(\n",
        "#       num_images, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRYOb6E4DesW"
      },
      "source": [
        "# Using the LINAC Defence\n",
        "\n",
        "Independent training of INRs for CIFAR-10 test-set images in order to compute their LINAC transforms. Sum squared encoding errors, averaged over pixels, are plotted against training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZFWP0i9_xpP"
      },
      "outputs": [],
      "source": [
        "# Instantiate LINAC Defence.\n",
        "defence = hk.without_apply_rng(hk.transform_with_state(linac_defence))\n",
        "\n",
        "\n",
        "def apply_linac(x):\n",
        "  return defence.apply({}, {}, x)[0]\n",
        "\n",
        "\n",
        "# Use LINAC to compute representations.\n",
        "linac_outputs = apply_linac(image_batch)\n",
        "\n",
        "\n",
        "# Use first output to construct a LINAC defended model.\n",
        "output_reps, params_and_states, train_stats = linac_outputs\n",
        "\n",
        "\n",
        "# Details of independent INR fitting processes for every input image.\n",
        "output_params, output_states, output_opt_states = params_and_states\n",
        "output_losses, output_norms, output_perms = train_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gjXbMlVyH5cO"
      },
      "outputs": [],
      "source": [
        "#@title Plots INR Training Losses\n",
        "\n",
        "\n",
        "colors = ['blue', 'orange', 'green', 'red',\n",
        "          'purple', 'yellow', 'cyan', 'brown']\n",
        "\n",
        "fig_h = 5\n",
        "fig_w = 10\n",
        "num_images_to_plot = max(image_batch.shape[0], len(colors))\n",
        "\n",
        "plt.figure(figsize=(fig_w, fig_h))\n",
        "for i in range(num_images_to_plot):\n",
        "  plt.plot(output_losses[i].reshape([-1]),\n",
        "           alpha=0.5, c=colors[i%len(colors)])\n",
        "plt.xlabel('training steps')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('sum squared errors (log-scale)')\n",
        "plt.grid(True)\n",
        "plt.title('Independent INR Training Losses per Input Image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHOfYmjUkH1V"
      },
      "source": [
        "## Reproducing **Figure 14** from the Appendix.\n",
        "Comparing transforms of the `3` top images using LINAC with the *private key*, as done for our defended classifier. The respective activation images with `H = 256` channels were plotted in a `16 × 16` grid of slices of the same size with original images. Respective slices over the channel dimension of activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations for the three input images (top). Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrZ-kUHnAP7O"
      },
      "outputs": [],
      "source": [
        "plot_linac_reps_on_color_channels(\n",
        "    image_batch, output_reps, image_indices=[0, 1, 2],\n",
        "    rows_per_layer=16, max_units_per_layer=256, plot_sz=1,\n",
        "    num_layers=1, num_units=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHxZFsDXC0yR"
      },
      "source": [
        "# Bonus: Visual inspection of outputs from INRs\n",
        "\n",
        "Independent training of INRs for CIFAR-10 test-set images in order to compute their approximate reconstructions. Sum squared encoding errors, averaged over pixels, are plotted against training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnsMjSY3AX0c"
      },
      "outputs": [],
      "source": [
        "# Instantiate LINAC Defence, but set the representation layer to be the\n",
        "# output layer of implicit networks in order to get reconstructions.\n",
        "reconstruction = hk.without_apply_rng(hk.transform_with_state(\n",
        "    functools.partial(linac_defence, representation_layer_index=5)))\n",
        "\n",
        "\n",
        "def linac_reconstruction(x):\n",
        "  return reconstruction.apply({}, {}, x)[0]\n",
        "\n",
        "\n",
        "# Use LINAC to compute reconstructions.\n",
        "reconstruction_outputs = linac_reconstruction(image_batch)\n",
        "\n",
        "\n",
        "# Use first output to visualize input image approximations.\n",
        "inr_output_images, params_and_states, train_stats = reconstruction_outputs\n",
        "\n",
        "\n",
        "# Details of independent INR fitting processes for every input image.\n",
        "output_params, output_states, output_opt_states = params_and_states\n",
        "output_losses, output_norms, output_perms = train_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wmgZOjM9H8Pp"
      },
      "outputs": [],
      "source": [
        "#@title Plots INR Training Losses\n",
        "\n",
        "\n",
        "colors = ['blue', 'orange', 'green', 'red',\n",
        "          'purple', 'yellow', 'cyan', 'brown']\n",
        "\n",
        "fig_h = 5\n",
        "fig_w = 10\n",
        "num_images_to_plot = max(image_batch.shape[0], len(colors))\n",
        "\n",
        "plt.figure(figsize=(fig_w, fig_h))\n",
        "for i in range(num_images_to_plot):\n",
        "  plt.plot(output_losses[i].reshape([-1]),\n",
        "           alpha=0.5, c=colors[i%len(colors)])\n",
        "plt.xlabel('training steps')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('sum squared errors (log-scale)')\n",
        "plt.grid(True)\n",
        "plt.title('Independent INR Training Losses per Input Image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv9UWKWEjIWE"
      },
      "source": [
        "## Reproducing **Figure 12** from the Appendix.\n",
        "\n",
        "Image approximations computed for LINAC with the *private key*, as used for our defended classifier. Original images and labels are plotted in the first column. Note that labels are not used for LINAC. Implicit network outputs are plotted in the second column. Difference images and sum squared errors, averaged over pixels, are plotted in the third column. Note that LINAC uses lossy image approximations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YJ5rgNc-Ycc"
      },
      "outputs": [],
      "source": [
        "compare_inputs_to_reconstructions(image_batch, inr_output_images, plot_sz=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtdU5rO0Acg-"
      },
      "source": [
        "#The End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}